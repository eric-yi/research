a <- c(1:100)
a.
a <- c(1, 100).
a <- c(1, 100).
a.
a
c <- c(1:100)
c
dir
100 %% 10
source('~/num_size.r')
source('~/num_size.r')
dir c/Users/
dir c:/Users
source('~/num_size.r')
5 / 10
source('~/num_size.r')
source('~/num_size.r')
source('~/num_size.r')
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
wdbc = read.csv("../../dataset/WDBC/WDBC.dat")
getwd()
setwd("reaseach")
setwd("reseach")
setwd("reseacher")
getpwd()
getwd()
setwd("researcher")
setwd("research")
setwd("prj/yi")
wdbc <- wdbc[2:32]
wdbc <- wdbc[2:32]
wdbc <- wdbc[2:32]
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
wdbc = read.csv("../../dataset/WDBC/WDBC.dat")
wdbc <- wdbc[2:32]
wdbc_n <- as.data.frame(lapply(wdbc[2:31], normlize))
wdbc_train <- wdbc_n[1:469, ]
wdbc_test <- wdbc_n[470:568, ]
wdbc_train_labels <- wdbc_n[1:469, 1]
wdbc_test_labels <- wdbc_n[470:569, 1]
library("class")
wdbc_pred <- knn(train=wdbc_train, test=wdbc_test, cl=wdbc_train_labels, k=3)
wdbc_pred
library("gmodels")
install.packages("gmodels")
load("gmodels")
library("gmodels")
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
CrossTable(x = wdbc_test_labels, y = wdbc_test_pred, prop.chisq = FALSE)
wbcd_test_pred <- knn(train=wdbc_train, test=wdbc_test, wdbc_train_labels, k = 21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
CrossTable(x = wdbc_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
wbcd_test_pred
CrossTable(x = wdbc_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
wdbc_test_labels
CrossTable(x = wdbc_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
wbcd_test_pred
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
wbcd_n = read.csv("../../dataset/wbcd/wbcd.dat")
CrossTable(x = wdbc_test_labels, y = wdbc_test_pred, prop.chisq = FALSE)
getwd()
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
wbcd_n = read.csv("../../dataset/wbcd/wbcd.dat")
getwd()
setwd("../r")
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
wbcd_n = read.csv("../../dataset/wbcd/wbcd.dat")
wbcd <- wbcd[2:32]
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
wdbc_n = read.csv("../../dataset/WDBC/WDBC.dat")
wdbc <- wdbc[2:32]
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
wdbc = read.csv("../../dataset/WDBC/WDBC.dat")
wdbc <- wdbc[2:32]
wdbc_n <- as.data.frame(lapply(wdbc[2:31], normlize))
wdbc_train <- wdbc_n[1:469, ]
wdbc_test <- wdbc_n[470:568, ]
wdbc_train_labels <- wdbc_n[1:469, 1]
wdbc_test_labels <- wdbc_n[470:569, 1]
library("class")
wdbc_pred <- knn(train=wdbc_train, test=wdbc_test, cl=wdbc_train_labels, k=3)
wdbc_test_pred <- knn(train=wdbc_train, test=wdbc_test, cl=wdbc_train_labels, k=21)
library("gmodels")
CrossTable(x=wdbc_test_labels, y=wdbc_test_pred, prop.chisq=FALSE)
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
wdbc = read.csv("../../dataset/WDBC/WDBC.dat")
wdbc <- wdbc[2:32]
wdbc_n <- as.data.frame(lapply(wdbc[2:31], normlize))
wdbc_train <- wdbc_n[1:469, ]
wdbc_test <- wdbc_n[470:568, ]
wdbc_train_labels <- wdbc_n[1:469, 1]
wdbc_test_labels <- wdbc_n[470:568, 1]
library("class")
wdbc_pred <- knn(train=wdbc_train, test=wdbc_test, cl=wdbc_train_labels, k=3)
wdbc_test_pred <- knn(train=wdbc_train, test=wdbc_test, cl=wdbc_train_labels, k=21)
library("gmodels")
CrossTable(x=wdbc_test_labels, y=wdbc_test_pred, prop.chisq=FALSE)
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
wdbc = read.csv("../../dataset/WDBC/WDBC.dat")
wdbc <- wdbc[2:32]
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
return ((x - min(x)) / (max(x) - min(x)) )
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
wdbc = read.csv("../../dataset/WDBC/WDBC.dat")
wdbc <- wdbc[2:32]
wdbc_n <- as.data.frame(lapply(wdbc[2:31], normlize))
wdbc_train <- wdbc_n[1:469, ]
wdbc_test <- wdbc_n[470:568, ]
wdbc_train_labels <- wdbc_n[1:469, 1]
wdbc_test_labels <- wdbc_n[470:568, 1]
library("class")
wdbc_pred <- knn(train=wdbc_train, test=wdbc_test, cl=wdbc_train_labels, k=3)
wdbc_test_pred <- knn(train=wdbc_train, test=wdbc_test, cl=wdbc_train_labels, k=21)
library("gmodels")
CrossTable(x=wdbc_test_labels, y=wdbc_test_pred, prop.chisq=FALSE)
#install.packages("readtext")
#install.packages("tm")
library(readtext)
library(stringr)
library(tm)
sms <- readtext("../../dataset/smspam/SMSSpamCollection.txt")
text1 <- str_remove(text, "Named chr ")
text <- unlist(sms[2])
text_list <- unlist(strsplit(text1, "\n"))
l <- length(text_list)
n <- 1
type = c()
text = c()
repeat {
line <- unlist(strsplit(text_list[n], "\t"))
type <- c(type, line[1])
text <- c(text, line[2])
if (n > l) {
break
}
n <- n + 1
}
sms_raw <- data.frame(type=type, text=text)
sms_raw$type <- factor(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
inspect(sms_corpus[1:3])
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
cospus_clean <- tm_map(corpus_clean, stripWhitespace)
inspect(corpus_clean[1:3])
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw_train <- sms_raw[1:4182, ]
sms_raw_test <- sms_raw[4183:5575, ]
sms_dtm_train <- sms_dtm[1:4182, ]
sms_dtm_test <- sms_dtm[4183:5574, ]
sms_corpus_train <- corpus_clean[1:4182]
sms_corpus_test <- corpus_clean[4183:5574]
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
#install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
#install.packages("readtext")
#install.packages("tm")
library(readtext)
library(stringr)
library(tm)
sms <- readtext("../../dataset/smspam/SMSSpamCollection.txt")
text <- unlist(sms[2])
text1 <- str_remove(text, "Named chr ")
text_list <- unlist(strsplit(text1, "\n"))
l <- length(text_list)
n <- 1
type = c()
text = c()
repeat {
line <- unlist(strsplit(text_list[n], "\t"))
type <- c(type, line[1])
text <- c(text, line[2])
if (n > l) {
break
}
n <- n + 1
}
sms_raw <- data.frame(type=type, text=text)
sms_raw$type <- factor(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
inspect(sms_corpus[1:3])
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
cospus_clean <- tm_map(corpus_clean, stripWhitespace)
inspect(corpus_clean[1:3])
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw_train <- sms_raw[1:4182, ]
sms_raw_test <- sms_raw[4183:5575, ]
sms_dtm_train <- sms_dtm[1:4182, ]
sms_dtm_test <- sms_dtm[4183:5574, ]
sms_corpus_train <- corpus_clean[1:4182]
sms_corpus_test <- corpus_clean[4183:5574]
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
#install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
spam <- subset(sms_raw_train, type=="spam")
ham <- subset(sms_raw_train, type="ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
View(sms_corpus)
findFreqTerms(sms_dtm_train, 5)
sms_dict <- Dictionary(findFreqTerms(sms_dtm_train, 5))
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary=sms_dict()))
library(Dicionary)
ham <- subset(sms_raw_train, type="ham")
library(Dictionary)
sms_dict <- findFreqTerms(sms_dtm_train, 5)
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary=sms_dict()))
sms_dict <- findFreqTerms(sms_dtm_train, 5)
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary=sms_dict()))
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary=sms_dict)
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary=sms_dict)
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary=sms_dict)
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary=sms_dict))
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary=sms_dict))
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary=sms_dict))
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x , levels=c(0,1), labels=c(""No"", ""Yes""))
return (x)
}
convert_counts <- function(x) {
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x , levels=c(0,1), labels=c("No", "Yes"))
return (x)
}
sms_train <- apply(sms_train, MARGIN=2, convert_counts)
sms_test <- apply(sms_test, MARGIN=2, convert_counts)
install.packages("e1071")
#install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, dnn=c('predicted', 'actual'))
CrossTable(sms_test_pred, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, dnn=c('predicted', 'actual'))
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
wdbc = read.csv("../../dataset/WDBC/WDBC.dat")
wdbc <- wdbc[2:32]
wdbc_n <- as.data.frame(lapply(wdbc[2:31], normlize))
wdbc_train <- wdbc_n[1:469, ]
wdbc_test <- wdbc_n[470:568, ]
wdbc_train_labels <- wdbc_n[1:469, 1]
wdbc_test_labels <- wdbc_n[470:568, 1]
library("class")
wdbc_pred <- knn(train=wdbc_train, test=wdbc_test, cl=wdbc_train_labels, k=3)
wdbc_test_pred <- knn(train=wdbc_train, test=wdbc_test, cl=wdbc_train_labels, k=21)
library("gmodels")
CrossTable(x=wdbc_test_labels, y=wdbc_test_pred, prop.chisq=FALSE)
getwd()
CrossTable(x=wdbc_test_labels, y=wdbc_test_pred, prop.chisq=FALSE)
''
""
''
)
CrossTable(sms_test_pred, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, dnn=c('predicted', 'actual'))
library(gmodels)
CrossTable(sms_test_pred, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, dnn=c('predicted', 'actual'))
normlize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)) )
}
wdbc = read.csv("../../dataset/WDBC/WDBC.dat")
wdbc <- wdbc[2:32]
wdbc_n <- as.data.frame(lapply(wdbc[2:31], normlize))
wdbc_train <- wdbc_n[1:469, ]
wdbc_test <- wdbc_n[470:568, ]
wdbc_train_labels <- wdbc_n[1:469, 1]
wdbc_test_labels <- wdbc_n[470:568, 1]
library("class")
wdbc_pred <- knn(train=wdbc_train, test=wdbc_test, cl=wdbc_train_labels, k=3)
wdbc_test_pred <- knn(train=wdbc_train, test=wdbc_test, cl=wdbc_train_labels, k=21)
library("gmodels")
CrossTable(x=wdbc_test_labels, y=wdbc_test_pred, prop.chisq=FALSE)
#install.packages("readtext")
#install.packages("tm")
library(readtext)
library(stringr)
library(tm)
sms <- readtext("../../dataset/smspam/SMSSpamCollection.txt")
text <- unlist(sms[2])
text_list <- unlist(strsplit(text1, "\n"))
text1 <- str_remove(text, "Named chr ")
l <- length(text_list)
n <- 1
type = c()
repeat {
line <- unlist(strsplit(text_list[n], "\t"))
type <- c(type, line[1])
text <- c(text, line[2])
if (n > l) {
break
}
n <- n + 1
}
l <- length(text_list)
n <- 1
type = c()
text = c()
repeat {
line <- unlist(strsplit(text_list[n], "\t"))
type <- c(type, line[1])
text <- c(text, line[2])
if (n > l) {
break
}
n <- n + 1
}
sms_raw <- data.frame(type=type, text=text)
sms_raw$type <- factor(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
inspect(sms_corpus[1:3])
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
cospus_clean <- tm_map(corpus_clean, stripWhitespace)
inspect(corpus_clean[1:3])
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw_train <- sms_raw[1:4182, ]
sms_raw_test <- sms_raw[4183:5575, ]
sms_dtm_train <- sms_dtm[1:4182, ]
sms_dtm_test <- sms_dtm[4183:5574, ]
sms_corpus_train <- corpus_clean[1:4182]
sms_corpus_test <- corpus_clean[4183:5574]
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
#install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
spam <- subset(sms_raw_train, type=="spam")
ham <- subset(sms_raw_train, type="ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
findFreqTerms(sms_dtm_train, 5)
sms_dict <- findFreqTerms(sms_dtm_train, 5)
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary=sms_dict))
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary=sms_dict))
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x , levels=c(0,1), labels=c("No", "Yes"))
return (x)
}
sms_train <- apply(sms_train, MARGIN=2, convert_counts)
sms_test <- apply(sms_test, MARGIN=2, convert_counts)
#install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_test_pred <- predict(sms_classifier, sms_test)
sms_test_pred <- predict(sms_classifier, sms_test)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
#install.packages("e1071")
library(e1071)
install.packages("e1071")
#library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
#install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, dnn=c('predicted', 'actual'))
length(sms_test_pred)
length(sms_raw_test$type)
#install.packages("readtext")
#install.packages("tm")
library(readtext)
library(stringr)
library(tm)
sms <- readtext("../../dataset/smspam/SMSSpamCollection.txt")
text <- unlist(sms[2])
text1 <- str_remove(text, "Named chr ")
text_list <- unlist(strsplit(text1, "\n"))
l <- length(text_list)
n <- 1
type = c()
text = c()
repeat {
line <- unlist(strsplit(text_list[n], "\t"))
type <- c(type, line[1])
text <- c(text, line[2])
if (n > l) {
break
}
n <- n + 1
}
sms_raw <- data.frame(type=type, text=text)
sms_raw$type <- factor(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
inspect(sms_corpus[1:3])
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
cospus_clean <- tm_map(corpus_clean, stripWhitespace)
inspect(corpus_clean[1:3])
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw_train <- sms_raw[1:4182, ]
sms_raw_test <- sms_raw[4183:5575, ]
sms_dtm_train <- sms_dtm[1:4182, ]
sms_dtm_test <- sms_dtm[4183:5575, ]
sms_corpus_train <- corpus_clean[1:4182]
sms_corpus_test <- corpus_clean[4183:5575]
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
#install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
spam <- subset(sms_raw_train, type=="spam")
ham <- subset(sms_raw_train, type="ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
findFreqTerms(sms_dtm_train, 5)
sms_dict <- findFreqTerms(sms_dtm_train, 5)
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary=sms_dict))
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary=sms_dict))
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x , levels=c(0,1), labels=c("No", "Yes"))
return (x)
}
sms_train <- apply(sms_train, MARGIN=2, convert_counts)
sms_test <- apply(sms_test, MARGIN=2, convert_counts)
#install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, dnn=c('predicted', 'actual'))
sms_classifier
View(sms_classifier)
CrossTable(sms_test_pred, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, dnn=c('predicted', 'actual'))
