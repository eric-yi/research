{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325a8110-ba15-43e5-94f5-60aaa1be789d",
   "metadata": {},
   "source": [
    "# Caffe Learning\n",
    "\n",
    "[https://caffe.berkeleyvision.org/](https://caffe.berkeleyvision.org/)\n",
    "\n",
    "[深度学习caffe入门到实战应用 b站](https://www.bilibili.com/video/BV1G4411Q7RF?spm_id_from=333.999.0.0)\n",
    "\n",
    "[OS X Installation](https://caffe.berkeleyvision.org/install_osx.html)\n",
    "\n",
    "<span style=\"color:red;\">*</span> [Install Caffe With Anaconda (Python 3 version)](https://yangcha.github.io/Caffe-Conda3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756503cf-9afc-4612-9079-7439a8cbe24e",
   "metadata": {},
   "source": [
    "[Solving in Python with LeNet官方教程](https://nbviewer.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b32d3-74ba-4fc9-9f8a-272a19e70ceb",
   "metadata": {},
   "source": [
    "## 数据类型\n",
    "\n",
    "- [LMDB: Lightning Memory-Mapped Database Manager](http://www.lmdb.tech/doc/)\n",
    "                                                 \n",
    "- [HDF5: The Hierarchical Data Format version 5](https://portal.hdfgroup.org/display/HDF5/HDF5)\n",
    "\n",
    "- [H5PY](https://www.h5py.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82fc582-9f51-4076-b937-2ab7d9497b63",
   "metadata": {},
   "source": [
    "## LeNet Mnist实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df23535e-36b4-4be5-827b-6da7fb66eed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20220425 15:23:50.897965 228398592 caffe.cpp:197] Use CPU.\n",
      "I20220425 15:23:50.898885 228398592 solver.cpp:45] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.01\n",
      "display: 100\n",
      "max_iter: 10000\n",
      "lr_policy: \"inv\"\n",
      "gamma: 0.0001\n",
      "power: 0.75\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "snapshot: 5000\n",
      "snapshot_prefix: \"mnist/lenet\"\n",
      "solver_mode: CPU\n",
      "net: \"mnist/lenet_train_test.prototxt\"\n",
      "train_state {\n",
      "  level: 0\n",
      "  stage: \"\"\n",
      "}\n",
      "I20220425 15:23:50.899070 228398592 solver.cpp:102] Creating training net from net file: mnist/lenet_train_test.prototxt\n",
      "I20220425 15:23:50.899634 228398592 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist\n",
      "I20220425 15:23:50.899654 228398592 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I20220425 15:23:50.899662 228398592 net.cpp:53] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "  level: 0\n",
      "  stage: \"\"\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"mnist/mnist_train_lmdb\"\n",
      "    batch_size: 64\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I20220425 15:23:50.900113 228398592 layer_factory.hpp:77] Creating layer mnist\n",
      "I20220425 15:23:50.900287 228398592 db_lmdb.cpp:35] Opened lmdb mnist/mnist_train_lmdb\n",
      "I20220425 15:23:50.900327 228398592 net.cpp:86] Creating Layer mnist\n",
      "I20220425 15:23:50.900337 228398592 net.cpp:382] mnist -> data\n",
      "I20220425 15:23:50.900353 228398592 net.cpp:382] mnist -> label\n",
      "I20220425 15:23:50.900380 228398592 data_layer.cpp:45] output data size: 64,1,28,28\n",
      "I20220425 15:23:50.901194 228398592 net.cpp:124] Setting up mnist\n",
      "I20220425 15:23:50.901206 228398592 net.cpp:131] Top shape: 64 1 28 28 (50176)\n",
      "I20220425 15:23:50.901213 228398592 net.cpp:131] Top shape: 64 (64)\n",
      "I20220425 15:23:50.901218 228398592 net.cpp:139] Memory required for data: 200960\n",
      "I20220425 15:23:50.901223 228398592 layer_factory.hpp:77] Creating layer conv1\n",
      "I20220425 15:23:50.901243 228398592 net.cpp:86] Creating Layer conv1\n",
      "I20220425 15:23:50.901249 228398592 net.cpp:408] conv1 <- data\n",
      "I20220425 15:23:50.901255 228398592 net.cpp:382] conv1 -> conv1\n",
      "I20220425 15:23:50.901304 228398592 net.cpp:124] Setting up conv1\n",
      "I20220425 15:23:50.901309 228398592 net.cpp:131] Top shape: 64 20 24 24 (737280)\n",
      "I20220425 15:23:50.901315 228398592 net.cpp:139] Memory required for data: 3150080\n",
      "I20220425 15:23:50.901327 228398592 layer_factory.hpp:77] Creating layer pool1\n",
      "I20220425 15:23:50.901404 228398592 net.cpp:86] Creating Layer pool1\n",
      "I20220425 15:23:50.901412 228398592 net.cpp:408] pool1 <- conv1\n",
      "I20220425 15:23:50.901417 228398592 net.cpp:382] pool1 -> pool1\n",
      "I20220425 15:23:50.901433 228398592 net.cpp:124] Setting up pool1\n",
      "I20220425 15:23:50.901437 228398592 net.cpp:131] Top shape: 64 20 12 12 (184320)\n",
      "I20220425 15:23:50.901443 228398592 net.cpp:139] Memory required for data: 3887360\n",
      "I20220425 15:23:50.901448 228398592 layer_factory.hpp:77] Creating layer conv2\n",
      "I20220425 15:23:50.901455 228398592 net.cpp:86] Creating Layer conv2\n",
      "I20220425 15:23:50.901460 228398592 net.cpp:408] conv2 <- pool1\n",
      "I20220425 15:23:50.901466 228398592 net.cpp:382] conv2 -> conv2\n",
      "I20220425 15:23:50.901685 228398592 net.cpp:124] Setting up conv2\n",
      "I20220425 15:23:50.901695 228398592 net.cpp:131] Top shape: 64 50 8 8 (204800)\n",
      "I20220425 15:23:50.901700 228398592 net.cpp:139] Memory required for data: 4706560\n",
      "I20220425 15:23:50.901707 228398592 layer_factory.hpp:77] Creating layer pool2\n",
      "I20220425 15:23:50.901715 228398592 net.cpp:86] Creating Layer pool2\n",
      "I20220425 15:23:50.901719 228398592 net.cpp:408] pool2 <- conv2\n",
      "I20220425 15:23:50.901724 228398592 net.cpp:382] pool2 -> pool2\n",
      "I20220425 15:23:50.901731 228398592 net.cpp:124] Setting up pool2\n",
      "I20220425 15:23:50.901736 228398592 net.cpp:131] Top shape: 64 50 4 4 (51200)\n",
      "I20220425 15:23:50.901742 228398592 net.cpp:139] Memory required for data: 4911360\n",
      "I20220425 15:23:50.901746 228398592 layer_factory.hpp:77] Creating layer ip1\n",
      "I20220425 15:23:50.901755 228398592 net.cpp:86] Creating Layer ip1\n",
      "I20220425 15:23:50.901759 228398592 net.cpp:408] ip1 <- pool2\n",
      "I20220425 15:23:50.901765 228398592 net.cpp:382] ip1 -> ip1\n",
      "I20220425 15:23:50.904991 228398592 net.cpp:124] Setting up ip1\n",
      "I20220425 15:23:50.905001 228398592 net.cpp:131] Top shape: 64 500 (32000)\n",
      "I20220425 15:23:50.905007 228398592 net.cpp:139] Memory required for data: 5039360\n",
      "I20220425 15:23:50.905014 228398592 layer_factory.hpp:77] Creating layer relu1\n",
      "I20220425 15:23:50.905021 228398592 net.cpp:86] Creating Layer relu1\n",
      "I20220425 15:23:50.905025 228398592 net.cpp:408] relu1 <- ip1\n",
      "I20220425 15:23:50.905030 228398592 net.cpp:369] relu1 -> ip1 (in-place)\n",
      "I20220425 15:23:50.905036 228398592 net.cpp:124] Setting up relu1\n",
      "I20220425 15:23:50.905040 228398592 net.cpp:131] Top shape: 64 500 (32000)\n",
      "I20220425 15:23:50.905045 228398592 net.cpp:139] Memory required for data: 5167360\n",
      "I20220425 15:23:50.905050 228398592 layer_factory.hpp:77] Creating layer ip2\n",
      "I20220425 15:23:50.905055 228398592 net.cpp:86] Creating Layer ip2\n",
      "I20220425 15:23:50.905059 228398592 net.cpp:408] ip2 <- ip1\n",
      "I20220425 15:23:50.905064 228398592 net.cpp:382] ip2 -> ip2\n",
      "I20220425 15:23:50.905107 228398592 net.cpp:124] Setting up ip2\n",
      "I20220425 15:23:50.905112 228398592 net.cpp:131] Top shape: 64 10 (640)\n",
      "I20220425 15:23:50.905117 228398592 net.cpp:139] Memory required for data: 5169920\n",
      "I20220425 15:23:50.905122 228398592 layer_factory.hpp:77] Creating layer loss\n",
      "I20220425 15:23:50.905128 228398592 net.cpp:86] Creating Layer loss\n",
      "I20220425 15:23:50.905133 228398592 net.cpp:408] loss <- ip2\n",
      "I20220425 15:23:50.905136 228398592 net.cpp:408] loss <- label\n",
      "I20220425 15:23:50.905141 228398592 net.cpp:382] loss -> loss\n",
      "I20220425 15:23:50.905153 228398592 layer_factory.hpp:77] Creating layer loss\n",
      "I20220425 15:23:50.905167 228398592 net.cpp:124] Setting up loss\n",
      "I20220425 15:23:50.905172 228398592 net.cpp:131] Top shape: (1)\n",
      "I20220425 15:23:50.905279 228398592 net.cpp:134]     with loss weight 1\n",
      "I20220425 15:23:50.905292 228398592 net.cpp:139] Memory required for data: 5169924\n",
      "I20220425 15:23:50.905298 228398592 net.cpp:200] loss needs backward computation.\n",
      "I20220425 15:23:50.905303 228398592 net.cpp:200] ip2 needs backward computation.\n",
      "I20220425 15:23:50.905308 228398592 net.cpp:200] relu1 needs backward computation.\n",
      "I20220425 15:23:50.905313 228398592 net.cpp:200] ip1 needs backward computation.\n",
      "I20220425 15:23:50.905318 228398592 net.cpp:200] pool2 needs backward computation.\n",
      "I20220425 15:23:50.905321 228398592 net.cpp:200] conv2 needs backward computation.\n",
      "I20220425 15:23:50.905395 228398592 net.cpp:200] pool1 needs backward computation.\n",
      "I20220425 15:23:50.905403 228398592 net.cpp:200] conv1 needs backward computation.\n",
      "I20220425 15:23:50.905408 228398592 net.cpp:202] mnist does not need backward computation.\n",
      "I20220425 15:23:50.905412 228398592 net.cpp:244] This network produces output loss\n",
      "I20220425 15:23:50.905419 228398592 net.cpp:257] Network initialization done.\n",
      "I20220425 15:23:50.905679 228398592 solver.cpp:190] Creating test net (#0) specified by net file: mnist/lenet_train_test.prototxt\n",
      "I20220425 15:23:50.905700 228398592 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I20220425 15:23:50.905710 228398592 net.cpp:53] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"mnist/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I20220425 15:23:50.906096 228398592 layer_factory.hpp:77] Creating layer mnist\n",
      "I20220425 15:23:50.906219 228398592 db_lmdb.cpp:35] Opened lmdb mnist/mnist_test_lmdb\n",
      "I20220425 15:23:50.906240 228398592 net.cpp:86] Creating Layer mnist\n",
      "I20220425 15:23:50.906247 228398592 net.cpp:382] mnist -> data\n",
      "I20220425 15:23:50.906256 228398592 net.cpp:382] mnist -> label\n",
      "I20220425 15:23:50.906272 228398592 data_layer.cpp:45] output data size: 100,1,28,28\n",
      "I20220425 15:23:50.906960 228398592 net.cpp:124] Setting up mnist\n",
      "I20220425 15:23:50.906971 228398592 net.cpp:131] Top shape: 100 1 28 28 (78400)\n",
      "I20220425 15:23:50.906978 228398592 net.cpp:131] Top shape: 100 (100)\n",
      "I20220425 15:23:50.906983 228398592 net.cpp:139] Memory required for data: 314000\n",
      "I20220425 15:23:50.906987 228398592 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I20220425 15:23:50.906996 228398592 net.cpp:86] Creating Layer label_mnist_1_split\n",
      "I20220425 15:23:50.906999 228398592 net.cpp:408] label_mnist_1_split <- label\n",
      "I20220425 15:23:50.907006 228398592 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I20220425 15:23:50.907012 228398592 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I20220425 15:23:50.907019 228398592 net.cpp:124] Setting up label_mnist_1_split\n",
      "I20220425 15:23:50.907076 228398592 net.cpp:131] Top shape: 100 (100)\n",
      "I20220425 15:23:50.907083 228398592 net.cpp:131] Top shape: 100 (100)\n",
      "I20220425 15:23:50.907088 228398592 net.cpp:139] Memory required for data: 314800\n",
      "I20220425 15:23:50.907094 228398592 layer_factory.hpp:77] Creating layer conv1\n",
      "I20220425 15:23:50.907101 228398592 net.cpp:86] Creating Layer conv1\n",
      "I20220425 15:23:50.907106 228398592 net.cpp:408] conv1 <- data\n",
      "I20220425 15:23:50.907111 228398592 net.cpp:382] conv1 -> conv1\n",
      "I20220425 15:23:50.907131 228398592 net.cpp:124] Setting up conv1\n",
      "I20220425 15:23:50.907137 228398592 net.cpp:131] Top shape: 100 20 24 24 (1152000)\n",
      "I20220425 15:23:50.907142 228398592 net.cpp:139] Memory required for data: 4922800\n",
      "I20220425 15:23:50.907150 228398592 layer_factory.hpp:77] Creating layer pool1\n",
      "I20220425 15:23:50.907155 228398592 net.cpp:86] Creating Layer pool1\n",
      "I20220425 15:23:50.907160 228398592 net.cpp:408] pool1 <- conv1\n",
      "I20220425 15:23:50.907205 228398592 net.cpp:382] pool1 -> pool1\n",
      "I20220425 15:23:50.907222 228398592 net.cpp:124] Setting up pool1\n",
      "I20220425 15:23:50.907229 228398592 net.cpp:131] Top shape: 100 20 12 12 (288000)\n",
      "I20220425 15:23:50.907236 228398592 net.cpp:139] Memory required for data: 6074800\n",
      "I20220425 15:23:50.907241 228398592 layer_factory.hpp:77] Creating layer conv2\n",
      "I20220425 15:23:50.907249 228398592 net.cpp:86] Creating Layer conv2\n",
      "I20220425 15:23:50.907254 228398592 net.cpp:408] conv2 <- pool1\n",
      "I20220425 15:23:50.907260 228398592 net.cpp:382] conv2 -> conv2\n",
      "I20220425 15:23:50.907476 228398592 net.cpp:124] Setting up conv2\n",
      "I20220425 15:23:50.907483 228398592 net.cpp:131] Top shape: 100 50 8 8 (320000)\n",
      "I20220425 15:23:50.907490 228398592 net.cpp:139] Memory required for data: 7354800\n",
      "I20220425 15:23:50.907497 228398592 layer_factory.hpp:77] Creating layer pool2\n",
      "I20220425 15:23:50.907503 228398592 net.cpp:86] Creating Layer pool2\n",
      "I20220425 15:23:50.907508 228398592 net.cpp:408] pool2 <- conv2\n",
      "I20220425 15:23:50.907513 228398592 net.cpp:382] pool2 -> pool2\n",
      "I20220425 15:23:50.907521 228398592 net.cpp:124] Setting up pool2\n",
      "I20220425 15:23:50.907526 228398592 net.cpp:131] Top shape: 100 50 4 4 (80000)\n",
      "I20220425 15:23:50.907531 228398592 net.cpp:139] Memory required for data: 7674800\n",
      "I20220425 15:23:50.907536 228398592 layer_factory.hpp:77] Creating layer ip1\n",
      "I20220425 15:23:50.907541 228398592 net.cpp:86] Creating Layer ip1\n",
      "I20220425 15:23:50.907546 228398592 net.cpp:408] ip1 <- pool2\n",
      "I20220425 15:23:50.907552 228398592 net.cpp:382] ip1 -> ip1\n",
      "I20220425 15:23:50.910692 228398592 net.cpp:124] Setting up ip1\n",
      "I20220425 15:23:50.910710 228398592 net.cpp:131] Top shape: 100 500 (50000)\n",
      "I20220425 15:23:50.910717 228398592 net.cpp:139] Memory required for data: 7874800\n",
      "I20220425 15:23:50.910725 228398592 layer_factory.hpp:77] Creating layer relu1\n",
      "I20220425 15:23:50.910732 228398592 net.cpp:86] Creating Layer relu1\n",
      "I20220425 15:23:50.910737 228398592 net.cpp:408] relu1 <- ip1\n",
      "I20220425 15:23:50.910749 228398592 net.cpp:369] relu1 -> ip1 (in-place)\n",
      "I20220425 15:23:50.910758 228398592 net.cpp:124] Setting up relu1\n",
      "I20220425 15:23:50.910761 228398592 net.cpp:131] Top shape: 100 500 (50000)\n",
      "I20220425 15:23:50.910768 228398592 net.cpp:139] Memory required for data: 8074800\n",
      "I20220425 15:23:50.910771 228398592 layer_factory.hpp:77] Creating layer ip2\n",
      "I20220425 15:23:50.910778 228398592 net.cpp:86] Creating Layer ip2\n",
      "I20220425 15:23:50.910784 228398592 net.cpp:408] ip2 <- ip1\n",
      "I20220425 15:23:50.910789 228398592 net.cpp:382] ip2 -> ip2\n",
      "I20220425 15:23:50.910840 228398592 net.cpp:124] Setting up ip2\n",
      "I20220425 15:23:50.910845 228398592 net.cpp:131] Top shape: 100 10 (1000)\n",
      "I20220425 15:23:50.910849 228398592 net.cpp:139] Memory required for data: 8078800\n",
      "I20220425 15:23:50.910856 228398592 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I20220425 15:23:50.910861 228398592 net.cpp:86] Creating Layer ip2_ip2_0_split\n",
      "I20220425 15:23:50.910866 228398592 net.cpp:408] ip2_ip2_0_split <- ip2\n",
      "I20220425 15:23:50.910871 228398592 net.cpp:382] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I20220425 15:23:50.910876 228398592 net.cpp:382] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I20220425 15:23:50.910950 228398592 net.cpp:124] Setting up ip2_ip2_0_split\n",
      "I20220425 15:23:50.910956 228398592 net.cpp:131] Top shape: 100 10 (1000)\n",
      "I20220425 15:23:50.910961 228398592 net.cpp:131] Top shape: 100 10 (1000)\n",
      "I20220425 15:23:50.910966 228398592 net.cpp:139] Memory required for data: 8086800\n",
      "I20220425 15:23:50.910970 228398592 layer_factory.hpp:77] Creating layer accuracy\n",
      "I20220425 15:23:50.910976 228398592 net.cpp:86] Creating Layer accuracy\n",
      "I20220425 15:23:50.910980 228398592 net.cpp:408] accuracy <- ip2_ip2_0_split_0\n",
      "I20220425 15:23:50.910985 228398592 net.cpp:408] accuracy <- label_mnist_1_split_0\n",
      "I20220425 15:23:50.910990 228398592 net.cpp:382] accuracy -> accuracy\n",
      "I20220425 15:23:50.911000 228398592 net.cpp:124] Setting up accuracy\n",
      "I20220425 15:23:50.911005 228398592 net.cpp:131] Top shape: (1)\n",
      "I20220425 15:23:50.911010 228398592 net.cpp:139] Memory required for data: 8086804\n",
      "I20220425 15:23:50.911015 228398592 layer_factory.hpp:77] Creating layer loss\n",
      "I20220425 15:23:50.911149 228398592 net.cpp:86] Creating Layer loss\n",
      "I20220425 15:23:50.911161 228398592 net.cpp:408] loss <- ip2_ip2_0_split_1\n",
      "I20220425 15:23:50.911168 228398592 net.cpp:408] loss <- label_mnist_1_split_1\n",
      "I20220425 15:23:50.911175 228398592 net.cpp:382] loss -> loss\n",
      "I20220425 15:23:50.911182 228398592 layer_factory.hpp:77] Creating layer loss\n",
      "I20220425 15:23:50.911192 228398592 net.cpp:124] Setting up loss\n",
      "I20220425 15:23:50.911197 228398592 net.cpp:131] Top shape: (1)\n",
      "I20220425 15:23:50.911202 228398592 net.cpp:134]     with loss weight 1\n",
      "I20220425 15:23:50.911207 228398592 net.cpp:139] Memory required for data: 8086808\n",
      "I20220425 15:23:50.911212 228398592 net.cpp:200] loss needs backward computation.\n",
      "I20220425 15:23:50.911216 228398592 net.cpp:202] accuracy does not need backward computation.\n",
      "I20220425 15:23:50.911221 228398592 net.cpp:200] ip2_ip2_0_split needs backward computation.\n",
      "I20220425 15:23:50.911226 228398592 net.cpp:200] ip2 needs backward computation.\n",
      "I20220425 15:23:50.911303 228398592 net.cpp:200] relu1 needs backward computation.\n",
      "I20220425 15:23:50.911311 228398592 net.cpp:200] ip1 needs backward computation.\n",
      "I20220425 15:23:50.911315 228398592 net.cpp:200] pool2 needs backward computation.\n",
      "I20220425 15:23:50.911319 228398592 net.cpp:200] conv2 needs backward computation.\n",
      "I20220425 15:23:50.911324 228398592 net.cpp:200] pool1 needs backward computation.\n",
      "I20220425 15:23:50.911329 228398592 net.cpp:200] conv1 needs backward computation.\n",
      "I20220425 15:23:50.911334 228398592 net.cpp:202] label_mnist_1_split does not need backward computation.\n",
      "I20220425 15:23:50.911339 228398592 net.cpp:202] mnist does not need backward computation.\n",
      "I20220425 15:23:50.911343 228398592 net.cpp:244] This network produces output accuracy\n",
      "I20220425 15:23:50.911347 228398592 net.cpp:244] This network produces output loss\n",
      "I20220425 15:23:50.911355 228398592 net.cpp:257] Network initialization done.\n",
      "I20220425 15:23:50.911387 228398592 solver.cpp:57] Solver scaffolding done.\n",
      "I20220425 15:23:50.911473 228398592 caffe.cpp:239] Starting Optimization\n",
      "I20220425 15:23:50.911479 228398592 solver.cpp:289] Solving LeNet\n",
      "I20220425 15:23:50.911484 228398592 solver.cpp:290] Learning Rate Policy: inv\n",
      "I20220425 15:23:50.912431 228398592 solver.cpp:347] Iteration 0, Testing net (#0)\n",
      "I20220425 15:23:53.048272 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:23:53.129516 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.0998\n",
      "I20220425 15:23:53.129624 228398592 solver.cpp:414]     Test net output #1: loss = 2.55024 (* 1 = 2.55024 loss)\n",
      "I20220425 15:23:53.162441 228398592 solver.cpp:239] Iteration 0 (0 iter/s, 2.25s/100 iters), loss = 2.49999\n",
      "I20220425 15:23:53.162474 228398592 solver.cpp:258]     Train net output #0: loss = 2.49999 (* 1 = 2.49999 loss)\n",
      "I20220425 15:23:53.162482 228398592 sgd_solver.cpp:112] Iteration 0, lr = 0.01\n",
      "I20220425 15:23:56.032570 228398592 solver.cpp:239] Iteration 100 (34.8432 iter/s, 2.87s/100 iters), loss = 0.208764\n",
      "I20220425 15:23:56.032610 228398592 solver.cpp:258]     Train net output #0: loss = 0.208764 (* 1 = 0.208764 loss)\n",
      "I20220425 15:23:56.032701 228398592 sgd_solver.cpp:112] Iteration 100, lr = 0.00992565\n",
      "I20220425 15:23:58.852604 228398592 solver.cpp:239] Iteration 200 (35.4736 iter/s, 2.819s/100 iters), loss = 0.158404\n",
      "I20220425 15:23:58.852630 228398592 solver.cpp:258]     Train net output #0: loss = 0.158404 (* 1 = 0.158404 loss)\n",
      "I20220425 15:23:58.852638 228398592 sgd_solver.cpp:112] Iteration 200, lr = 0.00985258\n",
      "I20220425 15:24:01.676668 228398592 solver.cpp:239] Iteration 300 (35.4108 iter/s, 2.824s/100 iters), loss = 0.151223\n",
      "I20220425 15:24:01.676695 228398592 solver.cpp:258]     Train net output #0: loss = 0.151223 (* 1 = 0.151223 loss)\n",
      "I20220425 15:24:01.676702 228398592 sgd_solver.cpp:112] Iteration 300, lr = 0.00978075\n",
      "I20220425 15:24:04.483116 228398592 solver.cpp:239] Iteration 400 (35.6379 iter/s, 2.806s/100 iters), loss = 0.0821742\n",
      "I20220425 15:24:04.483148 228398592 solver.cpp:258]     Train net output #0: loss = 0.0821742 (* 1 = 0.0821742 loss)\n",
      "I20220425 15:24:04.483157 228398592 sgd_solver.cpp:112] Iteration 400, lr = 0.00971013\n",
      "I20220425 15:24:07.125550 228398592 solver.cpp:347] Iteration 500, Testing net (#0)\n",
      "I20220425 15:24:09.072268 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:24:09.150264 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9728\n",
      "I20220425 15:24:09.150295 228398592 solver.cpp:414]     Test net output #1: loss = 0.0829228 (* 1 = 0.0829228 loss)\n",
      "I20220425 15:24:09.175734 228398592 solver.cpp:239] Iteration 500 (21.3129 iter/s, 4.692s/100 iters), loss = 0.113433\n",
      "I20220425 15:24:09.175763 228398592 solver.cpp:258]     Train net output #0: loss = 0.113433 (* 1 = 0.113433 loss)\n",
      "I20220425 15:24:09.175770 228398592 sgd_solver.cpp:112] Iteration 500, lr = 0.00964069\n",
      "I20220425 15:24:11.772459 228398592 solver.cpp:239] Iteration 600 (38.5208 iter/s, 2.596s/100 iters), loss = 0.0739261\n",
      "I20220425 15:24:11.772496 228398592 solver.cpp:258]     Train net output #0: loss = 0.0739261 (* 1 = 0.0739261 loss)\n",
      "I20220425 15:24:11.772506 228398592 sgd_solver.cpp:112] Iteration 600, lr = 0.0095724\n",
      "I20220425 15:24:14.380831 228398592 solver.cpp:239] Iteration 700 (38.3436 iter/s, 2.608s/100 iters), loss = 0.146681\n",
      "I20220425 15:24:14.380858 228398592 solver.cpp:258]     Train net output #0: loss = 0.146681 (* 1 = 0.146681 loss)\n",
      "I20220425 15:24:14.380865 228398592 sgd_solver.cpp:112] Iteration 700, lr = 0.00950522\n",
      "I20220425 15:24:16.981930 228398592 solver.cpp:239] Iteration 800 (38.4468 iter/s, 2.601s/100 iters), loss = 0.217597\n",
      "I20220425 15:24:16.981956 228398592 solver.cpp:258]     Train net output #0: loss = 0.217597 (* 1 = 0.217597 loss)\n",
      "I20220425 15:24:16.981963 228398592 sgd_solver.cpp:112] Iteration 800, lr = 0.00943913\n",
      "I20220425 15:24:19.575273 228398592 solver.cpp:239] Iteration 900 (38.5654 iter/s, 2.593s/100 iters), loss = 0.153747\n",
      "I20220425 15:24:19.575309 228398592 solver.cpp:258]     Train net output #0: loss = 0.153747 (* 1 = 0.153747 loss)\n",
      "I20220425 15:24:19.575317 228398592 sgd_solver.cpp:112] Iteration 900, lr = 0.00937411\n",
      "I20220425 15:24:20.446252 226213888 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:24:22.155287 228398592 solver.cpp:347] Iteration 1000, Testing net (#0)\n",
      "I20220425 15:24:24.097676 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:24:24.176368 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9818\n",
      "I20220425 15:24:24.176401 228398592 solver.cpp:414]     Test net output #1: loss = 0.0554563 (* 1 = 0.0554563 loss)\n",
      "I20220425 15:24:24.202184 228398592 solver.cpp:239] Iteration 1000 (21.6169 iter/s, 4.626s/100 iters), loss = 0.0667691\n",
      "I20220425 15:24:24.202214 228398592 solver.cpp:258]     Train net output #0: loss = 0.0667691 (* 1 = 0.0667691 loss)\n",
      "I20220425 15:24:24.202221 228398592 sgd_solver.cpp:112] Iteration 1000, lr = 0.00931012\n",
      "I20220425 15:24:26.801075 228398592 solver.cpp:239] Iteration 1100 (38.4911 iter/s, 2.598s/100 iters), loss = 0.0060592\n",
      "I20220425 15:24:26.801105 228398592 solver.cpp:258]     Train net output #0: loss = 0.00605916 (* 1 = 0.00605916 loss)\n",
      "I20220425 15:24:26.801113 228398592 sgd_solver.cpp:112] Iteration 1100, lr = 0.00924715\n",
      "I20220425 15:24:29.381389 228398592 solver.cpp:239] Iteration 1200 (38.7597 iter/s, 2.58s/100 iters), loss = 0.0326948\n",
      "I20220425 15:24:29.381417 228398592 solver.cpp:258]     Train net output #0: loss = 0.0326947 (* 1 = 0.0326947 loss)\n",
      "I20220425 15:24:29.381423 228398592 sgd_solver.cpp:112] Iteration 1200, lr = 0.00918515\n",
      "I20220425 15:24:32.008159 228398592 solver.cpp:239] Iteration 1300 (38.0807 iter/s, 2.626s/100 iters), loss = 0.0196351\n",
      "I20220425 15:24:32.008189 228398592 solver.cpp:258]     Train net output #0: loss = 0.019635 (* 1 = 0.019635 loss)\n",
      "I20220425 15:24:32.008196 228398592 sgd_solver.cpp:112] Iteration 1300, lr = 0.00912412\n",
      "I20220425 15:24:34.611932 228398592 solver.cpp:239] Iteration 1400 (38.4172 iter/s, 2.603s/100 iters), loss = 0.00488347\n",
      "I20220425 15:24:34.611964 228398592 solver.cpp:258]     Train net output #0: loss = 0.00488342 (* 1 = 0.00488342 loss)\n",
      "I20220425 15:24:34.611972 228398592 sgd_solver.cpp:112] Iteration 1400, lr = 0.00906403\n",
      "I20220425 15:24:37.167083 228398592 solver.cpp:347] Iteration 1500, Testing net (#0)\n",
      "I20220425 15:24:39.114593 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:24:39.192423 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9851\n",
      "I20220425 15:24:39.192452 228398592 solver.cpp:414]     Test net output #1: loss = 0.0471463 (* 1 = 0.0471463 loss)\n",
      "I20220425 15:24:39.217428 228398592 solver.cpp:239] Iteration 1500 (21.7155 iter/s, 4.605s/100 iters), loss = 0.0918662\n",
      "I20220425 15:24:39.217458 228398592 solver.cpp:258]     Train net output #0: loss = 0.0918661 (* 1 = 0.0918661 loss)\n",
      "I20220425 15:24:39.217464 228398592 sgd_solver.cpp:112] Iteration 1500, lr = 0.00900485\n",
      "I20220425 15:24:41.825657 228398592 solver.cpp:239] Iteration 1600 (38.3436 iter/s, 2.608s/100 iters), loss = 0.10798\n",
      "I20220425 15:24:41.825686 228398592 solver.cpp:258]     Train net output #0: loss = 0.10798 (* 1 = 0.10798 loss)\n",
      "I20220425 15:24:41.825693 228398592 sgd_solver.cpp:112] Iteration 1600, lr = 0.00894657\n",
      "I20220425 15:24:44.430701 228398592 solver.cpp:239] Iteration 1700 (38.3877 iter/s, 2.605s/100 iters), loss = 0.0329527\n",
      "I20220425 15:24:44.430737 228398592 solver.cpp:258]     Train net output #0: loss = 0.0329526 (* 1 = 0.0329526 loss)\n",
      "I20220425 15:24:44.430747 228398592 sgd_solver.cpp:112] Iteration 1700, lr = 0.00888916\n",
      "I20220425 15:24:47.016291 228398592 solver.cpp:239] Iteration 1800 (38.6847 iter/s, 2.585s/100 iters), loss = 0.0142687\n",
      "I20220425 15:24:47.016319 228398592 solver.cpp:258]     Train net output #0: loss = 0.0142686 (* 1 = 0.0142686 loss)\n",
      "I20220425 15:24:47.016326 228398592 sgd_solver.cpp:112] Iteration 1800, lr = 0.0088326\n",
      "I20220425 15:24:48.894858 226213888 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:24:49.671329 228398592 solver.cpp:239] Iteration 1900 (37.6648 iter/s, 2.655s/100 iters), loss = 0.129072\n",
      "I20220425 15:24:49.671356 228398592 solver.cpp:258]     Train net output #0: loss = 0.129071 (* 1 = 0.129071 loss)\n",
      "I20220425 15:24:49.671362 228398592 sgd_solver.cpp:112] Iteration 1900, lr = 0.00877687\n",
      "I20220425 15:24:52.259282 228398592 solver.cpp:347] Iteration 2000, Testing net (#0)\n",
      "I20220425 15:24:54.189241 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:24:54.267431 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9843\n",
      "I20220425 15:24:54.267463 228398592 solver.cpp:414]     Test net output #1: loss = 0.0465587 (* 1 = 0.0465587 loss)\n",
      "I20220425 15:24:54.292526 228398592 solver.cpp:239] Iteration 2000 (21.6403 iter/s, 4.621s/100 iters), loss = 0.0199928\n",
      "I20220425 15:24:54.292557 228398592 solver.cpp:258]     Train net output #0: loss = 0.0199927 (* 1 = 0.0199927 loss)\n",
      "I20220425 15:24:54.292564 228398592 sgd_solver.cpp:112] Iteration 2000, lr = 0.00872196\n",
      "I20220425 15:24:56.903983 228398592 solver.cpp:239] Iteration 2100 (38.2995 iter/s, 2.611s/100 iters), loss = 0.0184758\n",
      "I20220425 15:24:56.904011 228398592 solver.cpp:258]     Train net output #0: loss = 0.0184757 (* 1 = 0.0184757 loss)\n",
      "I20220425 15:24:56.904019 228398592 sgd_solver.cpp:112] Iteration 2100, lr = 0.00866784\n",
      "I20220425 15:24:59.530478 228398592 solver.cpp:239] Iteration 2200 (38.0807 iter/s, 2.626s/100 iters), loss = 0.0162571\n",
      "I20220425 15:24:59.530505 228398592 solver.cpp:258]     Train net output #0: loss = 0.016257 (* 1 = 0.016257 loss)\n",
      "I20220425 15:24:59.530512 228398592 sgd_solver.cpp:112] Iteration 2200, lr = 0.0086145\n",
      "I20220425 15:25:02.169453 228398592 solver.cpp:239] Iteration 2300 (37.9075 iter/s, 2.638s/100 iters), loss = 0.0899583\n",
      "I20220425 15:25:02.169489 228398592 solver.cpp:258]     Train net output #0: loss = 0.0899582 (* 1 = 0.0899582 loss)\n",
      "I20220425 15:25:02.169499 228398592 sgd_solver.cpp:112] Iteration 2300, lr = 0.00856192\n",
      "I20220425 15:25:04.770398 228398592 solver.cpp:239] Iteration 2400 (38.4615 iter/s, 2.6s/100 iters), loss = 0.00717611\n",
      "I20220425 15:25:04.770426 228398592 solver.cpp:258]     Train net output #0: loss = 0.00717593 (* 1 = 0.00717593 loss)\n",
      "I20220425 15:25:04.770434 228398592 sgd_solver.cpp:112] Iteration 2400, lr = 0.00851008\n",
      "I20220425 15:25:07.346539 228398592 solver.cpp:347] Iteration 2500, Testing net (#0)\n",
      "I20220425 15:25:09.273059 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:25:09.361025 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9855\n",
      "I20220425 15:25:09.361057 228398592 solver.cpp:414]     Test net output #1: loss = 0.0457167 (* 1 = 0.0457167 loss)\n",
      "I20220425 15:25:09.386850 228398592 solver.cpp:239] Iteration 2500 (21.6638 iter/s, 4.616s/100 iters), loss = 0.0264011\n",
      "I20220425 15:25:09.386879 228398592 solver.cpp:258]     Train net output #0: loss = 0.0264009 (* 1 = 0.0264009 loss)\n",
      "I20220425 15:25:09.386886 228398592 sgd_solver.cpp:112] Iteration 2500, lr = 0.00845897\n",
      "I20220425 15:25:12.077009 228398592 solver.cpp:239] Iteration 2600 (37.1747 iter/s, 2.69s/100 iters), loss = 0.0518977\n",
      "I20220425 15:25:12.077044 228398592 solver.cpp:258]     Train net output #0: loss = 0.0518975 (* 1 = 0.0518975 loss)\n",
      "I20220425 15:25:12.077052 228398592 sgd_solver.cpp:112] Iteration 2600, lr = 0.00840857\n",
      "I20220425 15:25:14.741570 228398592 solver.cpp:239] Iteration 2700 (37.5375 iter/s, 2.664s/100 iters), loss = 0.0484167\n",
      "I20220425 15:25:14.741598 228398592 solver.cpp:258]     Train net output #0: loss = 0.0484165 (* 1 = 0.0484165 loss)\n",
      "I20220425 15:25:14.741606 228398592 sgd_solver.cpp:112] Iteration 2700, lr = 0.00835886\n",
      "I20220425 15:25:17.393496 228398592 solver.cpp:239] Iteration 2800 (37.7216 iter/s, 2.651s/100 iters), loss = 0.0021871\n",
      "I20220425 15:25:17.393525 228398592 solver.cpp:258]     Train net output #0: loss = 0.00218693 (* 1 = 0.00218693 loss)\n",
      "I20220425 15:25:17.393532 228398592 sgd_solver.cpp:112] Iteration 2800, lr = 0.00830984\n",
      "I20220425 15:25:17.670017 226213888 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:25:20.358769 228398592 solver.cpp:239] Iteration 2900 (33.7268 iter/s, 2.965s/100 iters), loss = 0.0230877\n",
      "I20220425 15:25:20.358800 228398592 solver.cpp:258]     Train net output #0: loss = 0.0230875 (* 1 = 0.0230875 loss)\n",
      "I20220425 15:25:20.358808 228398592 sgd_solver.cpp:112] Iteration 2900, lr = 0.00826148\n",
      "I20220425 15:25:22.976627 228398592 solver.cpp:347] Iteration 3000, Testing net (#0)\n",
      "I20220425 15:25:24.899394 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:25:24.978350 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.987\n",
      "I20220425 15:25:24.978384 228398592 solver.cpp:414]     Test net output #1: loss = 0.0395608 (* 1 = 0.0395608 loss)\n",
      "I20220425 15:25:25.007966 228398592 solver.cpp:239] Iteration 3000 (21.51 iter/s, 4.649s/100 iters), loss = 0.0198254\n",
      "I20220425 15:25:25.008002 228398592 solver.cpp:258]     Train net output #0: loss = 0.0198252 (* 1 = 0.0198252 loss)\n",
      "I20220425 15:25:25.008011 228398592 sgd_solver.cpp:112] Iteration 3000, lr = 0.00821377\n",
      "I20220425 15:25:27.638839 228398592 solver.cpp:239] Iteration 3100 (38.0228 iter/s, 2.63s/100 iters), loss = 0.00625958\n",
      "I20220425 15:25:27.638866 228398592 solver.cpp:258]     Train net output #0: loss = 0.00625939 (* 1 = 0.00625939 loss)\n",
      "I20220425 15:25:27.638873 228398592 sgd_solver.cpp:112] Iteration 3100, lr = 0.0081667\n",
      "I20220425 15:25:30.560353 228398592 solver.cpp:239] Iteration 3200 (34.2349 iter/s, 2.921s/100 iters), loss = 0.00976935\n",
      "I20220425 15:25:30.560386 228398592 solver.cpp:258]     Train net output #0: loss = 0.00976915 (* 1 = 0.00976915 loss)\n",
      "I20220425 15:25:30.560395 228398592 sgd_solver.cpp:112] Iteration 3200, lr = 0.00812025\n",
      "I20220425 15:25:33.241503 228398592 solver.cpp:239] Iteration 3300 (37.2995 iter/s, 2.681s/100 iters), loss = 0.0112832\n",
      "I20220425 15:25:33.241531 228398592 solver.cpp:258]     Train net output #0: loss = 0.011283 (* 1 = 0.011283 loss)\n",
      "I20220425 15:25:33.241539 228398592 sgd_solver.cpp:112] Iteration 3300, lr = 0.00807442\n",
      "I20220425 15:25:35.902490 228398592 solver.cpp:239] Iteration 3400 (37.594 iter/s, 2.66s/100 iters), loss = 0.0141322\n",
      "I20220425 15:25:35.902519 228398592 solver.cpp:258]     Train net output #0: loss = 0.014132 (* 1 = 0.014132 loss)\n",
      "I20220425 15:25:35.902526 228398592 sgd_solver.cpp:112] Iteration 3400, lr = 0.00802918\n",
      "I20220425 15:25:38.523703 228398592 solver.cpp:347] Iteration 3500, Testing net (#0)\n",
      "I20220425 15:25:40.546268 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:25:40.643431 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9853\n",
      "I20220425 15:25:40.643460 228398592 solver.cpp:414]     Test net output #1: loss = 0.0401029 (* 1 = 0.0401029 loss)\n",
      "I20220425 15:25:40.677209 228398592 solver.cpp:239] Iteration 3500 (20.9468 iter/s, 4.774s/100 iters), loss = 0.0070466\n",
      "I20220425 15:25:40.677245 228398592 solver.cpp:258]     Train net output #0: loss = 0.0070464 (* 1 = 0.0070464 loss)\n",
      "I20220425 15:25:40.677256 228398592 sgd_solver.cpp:112] Iteration 3500, lr = 0.00798454\n",
      "I20220425 15:25:43.749197 228398592 solver.cpp:239] Iteration 3600 (32.5627 iter/s, 3.071s/100 iters), loss = 0.0267478\n",
      "I20220425 15:25:43.749223 228398592 solver.cpp:258]     Train net output #0: loss = 0.0267476 (* 1 = 0.0267476 loss)\n",
      "I20220425 15:25:43.749231 228398592 sgd_solver.cpp:112] Iteration 3600, lr = 0.00794046\n",
      "I20220425 15:25:46.378966 228398592 solver.cpp:239] Iteration 3700 (38.0373 iter/s, 2.629s/100 iters), loss = 0.0191631\n",
      "I20220425 15:25:46.378993 228398592 solver.cpp:258]     Train net output #0: loss = 0.0191629 (* 1 = 0.0191629 loss)\n",
      "I20220425 15:25:46.378999 228398592 sgd_solver.cpp:112] Iteration 3700, lr = 0.00789695\n",
      "I20220425 15:25:47.568871 226213888 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:25:49.013978 228398592 solver.cpp:239] Iteration 3800 (37.9651 iter/s, 2.634s/100 iters), loss = 0.00849684\n",
      "I20220425 15:25:49.014004 228398592 solver.cpp:258]     Train net output #0: loss = 0.00849658 (* 1 = 0.00849658 loss)\n",
      "I20220425 15:25:49.014011 228398592 sgd_solver.cpp:112] Iteration 3800, lr = 0.007854\n",
      "I20220425 15:25:51.636631 228398592 solver.cpp:239] Iteration 3900 (38.1388 iter/s, 2.622s/100 iters), loss = 0.0299604\n",
      "I20220425 15:25:51.636657 228398592 solver.cpp:258]     Train net output #0: loss = 0.0299602 (* 1 = 0.0299602 loss)\n",
      "I20220425 15:25:51.636665 228398592 sgd_solver.cpp:112] Iteration 3900, lr = 0.00781158\n",
      "I20220425 15:25:54.467911 228398592 solver.cpp:347] Iteration 4000, Testing net (#0)\n",
      "I20220425 15:25:56.393239 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:25:56.471148 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9888\n",
      "I20220425 15:25:56.471179 228398592 solver.cpp:414]     Test net output #1: loss = 0.0308387 (* 1 = 0.0308387 loss)\n",
      "I20220425 15:25:56.496534 228398592 solver.cpp:239] Iteration 4000 (20.5804 iter/s, 4.859s/100 iters), loss = 0.0216047\n",
      "I20220425 15:25:56.496565 228398592 solver.cpp:258]     Train net output #0: loss = 0.0216044 (* 1 = 0.0216044 loss)\n",
      "I20220425 15:25:56.496573 228398592 sgd_solver.cpp:112] Iteration 4000, lr = 0.00776969\n",
      "I20220425 15:25:59.083191 228398592 solver.cpp:239] Iteration 4100 (38.6698 iter/s, 2.586s/100 iters), loss = 0.033805\n",
      "I20220425 15:25:59.083220 228398592 solver.cpp:258]     Train net output #0: loss = 0.0338048 (* 1 = 0.0338048 loss)\n",
      "I20220425 15:25:59.083226 228398592 sgd_solver.cpp:112] Iteration 4100, lr = 0.00772833\n",
      "I20220425 15:26:01.681710 228398592 solver.cpp:239] Iteration 4200 (38.4911 iter/s, 2.598s/100 iters), loss = 0.0200764\n",
      "I20220425 15:26:01.681739 228398592 solver.cpp:258]     Train net output #0: loss = 0.0200761 (* 1 = 0.0200761 loss)\n",
      "I20220425 15:26:01.681747 228398592 sgd_solver.cpp:112] Iteration 4200, lr = 0.00768748\n",
      "I20220425 15:26:04.345332 228398592 solver.cpp:239] Iteration 4300 (37.5516 iter/s, 2.663s/100 iters), loss = 0.0491457\n",
      "I20220425 15:26:04.345366 228398592 solver.cpp:258]     Train net output #0: loss = 0.0491454 (* 1 = 0.0491454 loss)\n",
      "I20220425 15:26:04.345376 228398592 sgd_solver.cpp:112] Iteration 4300, lr = 0.00764712\n",
      "I20220425 15:26:07.010438 228398592 solver.cpp:239] Iteration 4400 (37.5235 iter/s, 2.665s/100 iters), loss = 0.0254368\n",
      "I20220425 15:26:07.010468 228398592 solver.cpp:258]     Train net output #0: loss = 0.0254365 (* 1 = 0.0254365 loss)\n",
      "I20220425 15:26:07.010475 228398592 sgd_solver.cpp:112] Iteration 4400, lr = 0.00760726\n",
      "I20220425 15:26:09.619930 228398592 solver.cpp:347] Iteration 4500, Testing net (#0)\n",
      "I20220425 15:26:11.595660 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:26:11.674243 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9874\n",
      "I20220425 15:26:11.674271 228398592 solver.cpp:414]     Test net output #1: loss = 0.0363144 (* 1 = 0.0363144 loss)\n",
      "I20220425 15:26:11.699648 228398592 solver.cpp:239] Iteration 4500 (21.3265 iter/s, 4.689s/100 iters), loss = 0.00678954\n",
      "I20220425 15:26:11.699676 228398592 solver.cpp:258]     Train net output #0: loss = 0.00678928 (* 1 = 0.00678928 loss)\n",
      "I20220425 15:26:11.699682 228398592 sgd_solver.cpp:112] Iteration 4500, lr = 0.00756788\n",
      "I20220425 15:26:14.340639 228398592 solver.cpp:239] Iteration 4600 (37.8788 iter/s, 2.64s/100 iters), loss = 0.00666426\n",
      "I20220425 15:26:14.340673 228398592 solver.cpp:258]     Train net output #0: loss = 0.00666399 (* 1 = 0.00666399 loss)\n",
      "I20220425 15:26:14.340683 228398592 sgd_solver.cpp:112] Iteration 4600, lr = 0.00752897\n",
      "I20220425 15:26:16.509924 226213888 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:26:16.941674 228398592 solver.cpp:239] Iteration 4700 (38.4468 iter/s, 2.601s/100 iters), loss = 0.00786932\n",
      "I20220425 15:26:16.941704 228398592 solver.cpp:258]     Train net output #0: loss = 0.00786906 (* 1 = 0.00786906 loss)\n",
      "I20220425 15:26:16.941711 228398592 sgd_solver.cpp:112] Iteration 4700, lr = 0.00749052\n",
      "I20220425 15:26:19.547849 228398592 solver.cpp:239] Iteration 4800 (38.373 iter/s, 2.606s/100 iters), loss = 0.0234242\n",
      "I20220425 15:26:19.547878 228398592 solver.cpp:258]     Train net output #0: loss = 0.023424 (* 1 = 0.023424 loss)\n",
      "I20220425 15:26:19.547885 228398592 sgd_solver.cpp:112] Iteration 4800, lr = 0.00745253\n",
      "I20220425 15:26:22.144708 228398592 solver.cpp:239] Iteration 4900 (38.5208 iter/s, 2.596s/100 iters), loss = 0.00697507\n",
      "I20220425 15:26:22.144745 228398592 solver.cpp:258]     Train net output #0: loss = 0.00697481 (* 1 = 0.00697481 loss)\n",
      "I20220425 15:26:22.144755 228398592 sgd_solver.cpp:112] Iteration 4900, lr = 0.00741498\n",
      "I20220425 15:26:24.705268 228398592 solver.cpp:464] Snapshotting to binary proto file mnist/lenet_iter_5000.caffemodel\n",
      "I20220425 15:26:24.715035 228398592 sgd_solver.cpp:284] Snapshotting solver state to binary proto file mnist/lenet_iter_5000.solverstate\n",
      "I20220425 15:26:24.725975 228398592 solver.cpp:347] Iteration 5000, Testing net (#0)\n",
      "I20220425 15:26:26.720082 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:26:26.797511 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9893\n",
      "I20220425 15:26:26.797541 228398592 solver.cpp:414]     Test net output #1: loss = 0.0320959 (* 1 = 0.0320959 loss)\n",
      "I20220425 15:26:26.822598 228398592 solver.cpp:239] Iteration 5000 (21.3812 iter/s, 4.677s/100 iters), loss = 0.038838\n",
      "I20220425 15:26:26.822629 228398592 solver.cpp:258]     Train net output #0: loss = 0.0388377 (* 1 = 0.0388377 loss)\n",
      "I20220425 15:26:26.822636 228398592 sgd_solver.cpp:112] Iteration 5000, lr = 0.00737788\n",
      "I20220425 15:26:29.416908 228398592 solver.cpp:239] Iteration 5100 (38.5505 iter/s, 2.594s/100 iters), loss = 0.0206514\n",
      "I20220425 15:26:29.416935 228398592 solver.cpp:258]     Train net output #0: loss = 0.0206512 (* 1 = 0.0206512 loss)\n",
      "I20220425 15:26:29.416942 228398592 sgd_solver.cpp:112] Iteration 5100, lr = 0.0073412\n",
      "I20220425 15:26:32.000958 228398592 solver.cpp:239] Iteration 5200 (38.6997 iter/s, 2.584s/100 iters), loss = 0.0074525\n",
      "I20220425 15:26:32.000990 228398592 solver.cpp:258]     Train net output #0: loss = 0.00745224 (* 1 = 0.00745224 loss)\n",
      "I20220425 15:26:32.000998 228398592 sgd_solver.cpp:112] Iteration 5200, lr = 0.00730495\n",
      "I20220425 15:26:34.595840 228398592 solver.cpp:239] Iteration 5300 (38.5505 iter/s, 2.594s/100 iters), loss = 0.00267327\n",
      "I20220425 15:26:34.595870 228398592 solver.cpp:258]     Train net output #0: loss = 0.00267302 (* 1 = 0.00267302 loss)\n",
      "I20220425 15:26:34.595878 228398592 sgd_solver.cpp:112] Iteration 5300, lr = 0.00726911\n",
      "I20220425 15:26:37.194058 228398592 solver.cpp:239] Iteration 5400 (38.4911 iter/s, 2.598s/100 iters), loss = 0.00675391\n",
      "I20220425 15:26:37.194087 228398592 solver.cpp:258]     Train net output #0: loss = 0.00675364 (* 1 = 0.00675364 loss)\n",
      "I20220425 15:26:37.194094 228398592 sgd_solver.cpp:112] Iteration 5400, lr = 0.00723368\n",
      "I20220425 15:26:39.829190 228398592 solver.cpp:347] Iteration 5500, Testing net (#0)\n",
      "I20220425 15:26:41.787616 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:26:41.865463 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9887\n",
      "I20220425 15:26:41.865491 228398592 solver.cpp:414]     Test net output #1: loss = 0.0323543 (* 1 = 0.0323543 loss)\n",
      "I20220425 15:26:41.891245 228398592 solver.cpp:239] Iteration 5500 (21.2902 iter/s, 4.697s/100 iters), loss = 0.00740343\n",
      "I20220425 15:26:41.891275 228398592 solver.cpp:258]     Train net output #0: loss = 0.00740316 (* 1 = 0.00740316 loss)\n",
      "I20220425 15:26:41.891283 228398592 sgd_solver.cpp:112] Iteration 5500, lr = 0.00719865\n",
      "I20220425 15:26:44.501555 228398592 solver.cpp:239] Iteration 5600 (38.3142 iter/s, 2.61s/100 iters), loss = 0.000769483\n",
      "I20220425 15:26:44.501586 228398592 solver.cpp:258]     Train net output #0: loss = 0.000769211 (* 1 = 0.000769211 loss)\n",
      "I20220425 15:26:44.501595 228398592 sgd_solver.cpp:112] Iteration 5600, lr = 0.00716402\n",
      "I20220425 15:26:45.024484 226213888 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:26:47.099151 228398592 solver.cpp:239] Iteration 5700 (38.506 iter/s, 2.597s/100 iters), loss = 0.00421509\n",
      "I20220425 15:26:47.099182 228398592 solver.cpp:258]     Train net output #0: loss = 0.00421482 (* 1 = 0.00421482 loss)\n",
      "I20220425 15:26:47.099190 228398592 sgd_solver.cpp:112] Iteration 5700, lr = 0.00712977\n",
      "I20220425 15:26:49.686600 228398592 solver.cpp:239] Iteration 5800 (38.6548 iter/s, 2.587s/100 iters), loss = 0.0318348\n",
      "I20220425 15:26:49.686630 228398592 solver.cpp:258]     Train net output #0: loss = 0.0318345 (* 1 = 0.0318345 loss)\n",
      "I20220425 15:26:49.686636 228398592 sgd_solver.cpp:112] Iteration 5800, lr = 0.0070959\n",
      "I20220425 15:26:52.284222 228398592 solver.cpp:239] Iteration 5900 (38.506 iter/s, 2.597s/100 iters), loss = 0.00567932\n",
      "I20220425 15:26:52.284250 228398592 solver.cpp:258]     Train net output #0: loss = 0.00567904 (* 1 = 0.00567904 loss)\n",
      "I20220425 15:26:52.284339 228398592 sgd_solver.cpp:112] Iteration 5900, lr = 0.0070624\n",
      "I20220425 15:26:54.855932 228398592 solver.cpp:347] Iteration 6000, Testing net (#0)\n",
      "I20220425 15:26:56.774592 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:26:56.856951 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9907\n",
      "I20220425 15:26:56.856986 228398592 solver.cpp:414]     Test net output #1: loss = 0.0281695 (* 1 = 0.0281695 loss)\n",
      "I20220425 15:26:56.885866 228398592 solver.cpp:239] Iteration 6000 (21.7344 iter/s, 4.601s/100 iters), loss = 0.00233122\n",
      "I20220425 15:26:56.885905 228398592 solver.cpp:258]     Train net output #0: loss = 0.00233094 (* 1 = 0.00233094 loss)\n",
      "I20220425 15:26:56.885915 228398592 sgd_solver.cpp:112] Iteration 6000, lr = 0.00702927\n",
      "I20220425 15:26:59.509478 228398592 solver.cpp:239] Iteration 6100 (38.1243 iter/s, 2.623s/100 iters), loss = 0.00305477\n",
      "I20220425 15:26:59.509510 228398592 solver.cpp:258]     Train net output #0: loss = 0.00305447 (* 1 = 0.00305447 loss)\n",
      "I20220425 15:26:59.509517 228398592 sgd_solver.cpp:112] Iteration 6100, lr = 0.0069965\n",
      "I20220425 15:27:02.217082 228398592 solver.cpp:239] Iteration 6200 (36.9413 iter/s, 2.707s/100 iters), loss = 0.00599494\n",
      "I20220425 15:27:02.217113 228398592 solver.cpp:258]     Train net output #0: loss = 0.00599464 (* 1 = 0.00599464 loss)\n",
      "I20220425 15:27:02.217121 228398592 sgd_solver.cpp:112] Iteration 6200, lr = 0.00696408\n",
      "I20220425 15:27:05.132210 228398592 solver.cpp:239] Iteration 6300 (34.3053 iter/s, 2.915s/100 iters), loss = 0.00716166\n",
      "I20220425 15:27:05.132237 228398592 solver.cpp:258]     Train net output #0: loss = 0.00716136 (* 1 = 0.00716136 loss)\n",
      "I20220425 15:27:05.132246 228398592 sgd_solver.cpp:112] Iteration 6300, lr = 0.00693201\n",
      "I20220425 15:27:07.736752 228398592 solver.cpp:239] Iteration 6400 (38.4025 iter/s, 2.604s/100 iters), loss = 0.00436682\n",
      "I20220425 15:27:07.736783 228398592 solver.cpp:258]     Train net output #0: loss = 0.00436651 (* 1 = 0.00436651 loss)\n",
      "I20220425 15:27:07.736789 228398592 sgd_solver.cpp:112] Iteration 6400, lr = 0.00690029\n",
      "I20220425 15:27:10.296602 228398592 solver.cpp:347] Iteration 6500, Testing net (#0)\n",
      "I20220425 15:27:12.239974 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:27:12.318248 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9897\n",
      "I20220425 15:27:12.318279 228398592 solver.cpp:414]     Test net output #1: loss = 0.0308023 (* 1 = 0.0308023 loss)\n",
      "I20220425 15:27:12.343327 228398592 solver.cpp:239] Iteration 6500 (21.7108 iter/s, 4.606s/100 iters), loss = 0.00543574\n",
      "I20220425 15:27:12.343358 228398592 solver.cpp:258]     Train net output #0: loss = 0.00543542 (* 1 = 0.00543542 loss)\n",
      "I20220425 15:27:12.343365 228398592 sgd_solver.cpp:112] Iteration 6500, lr = 0.0068689\n",
      "I20220425 15:27:13.882230 226213888 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:27:14.976907 228398592 solver.cpp:239] Iteration 6600 (37.9795 iter/s, 2.633s/100 iters), loss = 0.0179849\n",
      "I20220425 15:27:14.976935 228398592 solver.cpp:258]     Train net output #0: loss = 0.0179846 (* 1 = 0.0179846 loss)\n",
      "I20220425 15:27:14.976943 228398592 sgd_solver.cpp:112] Iteration 6600, lr = 0.00683784\n",
      "I20220425 15:27:17.648628 228398592 solver.cpp:239] Iteration 6700 (37.4392 iter/s, 2.671s/100 iters), loss = 0.011158\n",
      "I20220425 15:27:17.648655 228398592 solver.cpp:258]     Train net output #0: loss = 0.0111577 (* 1 = 0.0111577 loss)\n",
      "I20220425 15:27:17.648663 228398592 sgd_solver.cpp:112] Iteration 6700, lr = 0.00680711\n",
      "I20220425 15:27:20.293493 228398592 solver.cpp:239] Iteration 6800 (37.8215 iter/s, 2.644s/100 iters), loss = 0.00263179\n",
      "I20220425 15:27:20.293530 228398592 solver.cpp:258]     Train net output #0: loss = 0.00263147 (* 1 = 0.00263147 loss)\n",
      "I20220425 15:27:20.293540 228398592 sgd_solver.cpp:112] Iteration 6800, lr = 0.0067767\n",
      "I20220425 15:27:22.927697 228398592 solver.cpp:239] Iteration 6900 (37.9651 iter/s, 2.634s/100 iters), loss = 0.00458698\n",
      "I20220425 15:27:22.927728 228398592 solver.cpp:258]     Train net output #0: loss = 0.00458666 (* 1 = 0.00458666 loss)\n",
      "I20220425 15:27:22.927736 228398592 sgd_solver.cpp:112] Iteration 6900, lr = 0.0067466\n",
      "I20220425 15:27:25.497305 228398592 solver.cpp:347] Iteration 7000, Testing net (#0)\n",
      "I20220425 15:27:27.433269 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:27:27.514927 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.989\n",
      "I20220425 15:27:27.514958 228398592 solver.cpp:414]     Test net output #1: loss = 0.0308409 (* 1 = 0.0308409 loss)\n",
      "I20220425 15:27:27.540277 228398592 solver.cpp:239] Iteration 7000 (21.6826 iter/s, 4.612s/100 iters), loss = 0.00401157\n",
      "I20220425 15:27:27.540306 228398592 solver.cpp:258]     Train net output #0: loss = 0.00401125 (* 1 = 0.00401125 loss)\n",
      "I20220425 15:27:27.540314 228398592 sgd_solver.cpp:112] Iteration 7000, lr = 0.00671681\n",
      "I20220425 15:27:30.119469 228398592 solver.cpp:239] Iteration 7100 (38.7747 iter/s, 2.579s/100 iters), loss = 0.020659\n",
      "I20220425 15:27:30.119499 228398592 solver.cpp:258]     Train net output #0: loss = 0.0206587 (* 1 = 0.0206587 loss)\n",
      "I20220425 15:27:30.119506 228398592 sgd_solver.cpp:112] Iteration 7100, lr = 0.00668733\n",
      "I20220425 15:27:33.021090 228398592 solver.cpp:239] Iteration 7200 (34.4709 iter/s, 2.901s/100 iters), loss = 0.00409661\n",
      "I20220425 15:27:33.021117 228398592 solver.cpp:258]     Train net output #0: loss = 0.00409629 (* 1 = 0.00409629 loss)\n",
      "I20220425 15:27:33.021124 228398592 sgd_solver.cpp:112] Iteration 7200, lr = 0.00665815\n",
      "I20220425 15:27:35.676822 228398592 solver.cpp:239] Iteration 7300 (37.6648 iter/s, 2.655s/100 iters), loss = 0.0174691\n",
      "I20220425 15:27:35.676851 228398592 solver.cpp:258]     Train net output #0: loss = 0.0174687 (* 1 = 0.0174687 loss)\n",
      "I20220425 15:27:35.676859 228398592 sgd_solver.cpp:112] Iteration 7300, lr = 0.00662927\n",
      "I20220425 15:27:38.368505 228398592 solver.cpp:239] Iteration 7400 (37.1609 iter/s, 2.691s/100 iters), loss = 0.00591287\n",
      "I20220425 15:27:38.368533 228398592 solver.cpp:258]     Train net output #0: loss = 0.00591255 (* 1 = 0.00591255 loss)\n",
      "I20220425 15:27:38.368541 228398592 sgd_solver.cpp:112] Iteration 7400, lr = 0.00660067\n",
      "I20220425 15:27:40.941298 226213888 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:27:41.043344 228398592 solver.cpp:347] Iteration 7500, Testing net (#0)\n",
      "I20220425 15:27:43.004235 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:27:43.096763 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9894\n",
      "I20220425 15:27:43.096796 228398592 solver.cpp:414]     Test net output #1: loss = 0.0314202 (* 1 = 0.0314202 loss)\n",
      "I20220425 15:27:43.122723 228398592 solver.cpp:239] Iteration 7500 (21.0349 iter/s, 4.754s/100 iters), loss = 0.001655\n",
      "I20220425 15:27:43.122752 228398592 solver.cpp:258]     Train net output #0: loss = 0.00165469 (* 1 = 0.00165469 loss)\n",
      "I20220425 15:27:43.122759 228398592 sgd_solver.cpp:112] Iteration 7500, lr = 0.00657236\n",
      "I20220425 15:27:45.733772 228398592 solver.cpp:239] Iteration 7600 (38.2995 iter/s, 2.611s/100 iters), loss = 0.00686053\n",
      "I20220425 15:27:45.733800 228398592 solver.cpp:258]     Train net output #0: loss = 0.00686022 (* 1 = 0.00686022 loss)\n",
      "I20220425 15:27:45.733808 228398592 sgd_solver.cpp:112] Iteration 7600, lr = 0.00654433\n",
      "I20220425 15:27:48.372047 228398592 solver.cpp:239] Iteration 7700 (37.9075 iter/s, 2.638s/100 iters), loss = 0.0256501\n",
      "I20220425 15:27:48.372077 228398592 solver.cpp:258]     Train net output #0: loss = 0.0256498 (* 1 = 0.0256498 loss)\n",
      "I20220425 15:27:48.372085 228398592 sgd_solver.cpp:112] Iteration 7700, lr = 0.00651658\n",
      "I20220425 15:27:50.976037 228398592 solver.cpp:239] Iteration 7800 (38.4172 iter/s, 2.603s/100 iters), loss = 0.00271734\n",
      "I20220425 15:27:50.976068 228398592 solver.cpp:258]     Train net output #0: loss = 0.00271703 (* 1 = 0.00271703 loss)\n",
      "I20220425 15:27:50.976075 228398592 sgd_solver.cpp:112] Iteration 7800, lr = 0.00648911\n",
      "I20220425 15:27:53.608824 228398592 solver.cpp:239] Iteration 7900 (37.9939 iter/s, 2.632s/100 iters), loss = 0.00305721\n",
      "I20220425 15:27:53.608851 228398592 solver.cpp:258]     Train net output #0: loss = 0.0030569 (* 1 = 0.0030569 loss)\n",
      "I20220425 15:27:53.608860 228398592 sgd_solver.cpp:112] Iteration 7900, lr = 0.0064619\n",
      "I20220425 15:27:56.217962 228398592 solver.cpp:347] Iteration 8000, Testing net (#0)\n",
      "I20220425 15:27:58.163873 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:27:58.241199 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.99\n",
      "I20220425 15:27:58.241230 228398592 solver.cpp:414]     Test net output #1: loss = 0.0295969 (* 1 = 0.0295969 loss)\n",
      "I20220425 15:27:58.266124 228398592 solver.cpp:239] Iteration 8000 (21.4731 iter/s, 4.657s/100 iters), loss = 0.00714815\n",
      "I20220425 15:27:58.266153 228398592 solver.cpp:258]     Train net output #0: loss = 0.00714784 (* 1 = 0.00714784 loss)\n",
      "I20220425 15:27:58.266160 228398592 sgd_solver.cpp:112] Iteration 8000, lr = 0.00643496\n",
      "I20220425 15:28:00.863778 228398592 solver.cpp:239] Iteration 8100 (38.506 iter/s, 2.597s/100 iters), loss = 0.00789271\n",
      "I20220425 15:28:00.863809 228398592 solver.cpp:258]     Train net output #0: loss = 0.00789241 (* 1 = 0.00789241 loss)\n",
      "I20220425 15:28:00.863817 228398592 sgd_solver.cpp:112] Iteration 8100, lr = 0.00640827\n",
      "I20220425 15:28:03.445474 228398592 solver.cpp:239] Iteration 8200 (38.7447 iter/s, 2.581s/100 iters), loss = 0.00977668\n",
      "I20220425 15:28:03.445504 228398592 solver.cpp:258]     Train net output #0: loss = 0.00977638 (* 1 = 0.00977638 loss)\n",
      "I20220425 15:28:03.445513 228398592 sgd_solver.cpp:112] Iteration 8200, lr = 0.00638185\n",
      "I20220425 15:28:06.049629 228398592 solver.cpp:239] Iteration 8300 (38.4025 iter/s, 2.604s/100 iters), loss = 0.037743\n",
      "I20220425 15:28:06.049657 228398592 solver.cpp:258]     Train net output #0: loss = 0.0377427 (* 1 = 0.0377427 loss)\n",
      "I20220425 15:28:06.049665 228398592 sgd_solver.cpp:112] Iteration 8300, lr = 0.00635568\n",
      "I20220425 15:28:08.652261 228398592 solver.cpp:239] Iteration 8400 (38.432 iter/s, 2.602s/100 iters), loss = 0.00425702\n",
      "I20220425 15:28:08.652289 228398592 solver.cpp:258]     Train net output #0: loss = 0.00425672 (* 1 = 0.00425672 loss)\n",
      "I20220425 15:28:08.652297 228398592 sgd_solver.cpp:112] Iteration 8400, lr = 0.00632975\n",
      "I20220425 15:28:09.501840 226213888 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:28:11.200736 228398592 solver.cpp:347] Iteration 8500, Testing net (#0)\n",
      "I20220425 15:28:13.153275 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:28:13.231614 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9906\n",
      "I20220425 15:28:13.231645 228398592 solver.cpp:414]     Test net output #1: loss = 0.0291085 (* 1 = 0.0291085 loss)\n",
      "I20220425 15:28:13.257678 228398592 solver.cpp:239] Iteration 8500 (21.7155 iter/s, 4.605s/100 iters), loss = 0.00482202\n",
      "I20220425 15:28:13.257705 228398592 solver.cpp:258]     Train net output #0: loss = 0.00482172 (* 1 = 0.00482172 loss)\n",
      "I20220425 15:28:13.257714 228398592 sgd_solver.cpp:112] Iteration 8500, lr = 0.00630407\n",
      "I20220425 15:28:15.846311 228398592 solver.cpp:239] Iteration 8600 (38.6399 iter/s, 2.588s/100 iters), loss = 0.000819672\n",
      "I20220425 15:28:15.846343 228398592 solver.cpp:258]     Train net output #0: loss = 0.000819378 (* 1 = 0.000819378 loss)\n",
      "I20220425 15:28:15.846350 228398592 sgd_solver.cpp:112] Iteration 8600, lr = 0.00627864\n",
      "I20220425 15:28:18.414880 228398592 solver.cpp:239] Iteration 8700 (38.9408 iter/s, 2.568s/100 iters), loss = 0.00291915\n",
      "I20220425 15:28:18.414917 228398592 solver.cpp:258]     Train net output #0: loss = 0.00291886 (* 1 = 0.00291886 loss)\n",
      "I20220425 15:28:18.414925 228398592 sgd_solver.cpp:112] Iteration 8700, lr = 0.00625344\n",
      "I20220425 15:28:21.011508 228398592 solver.cpp:239] Iteration 8800 (38.5208 iter/s, 2.596s/100 iters), loss = 0.00108172\n",
      "I20220425 15:28:21.011535 228398592 solver.cpp:258]     Train net output #0: loss = 0.00108143 (* 1 = 0.00108143 loss)\n",
      "I20220425 15:28:21.011543 228398592 sgd_solver.cpp:112] Iteration 8800, lr = 0.00622847\n",
      "I20220425 15:28:23.609383 228398592 solver.cpp:239] Iteration 8900 (38.506 iter/s, 2.597s/100 iters), loss = 0.000379224\n",
      "I20220425 15:28:23.609411 228398592 solver.cpp:258]     Train net output #0: loss = 0.000378928 (* 1 = 0.000378928 loss)\n",
      "I20220425 15:28:23.609417 228398592 sgd_solver.cpp:112] Iteration 8900, lr = 0.00620374\n",
      "I20220425 15:28:26.155548 228398592 solver.cpp:347] Iteration 9000, Testing net (#0)\n",
      "I20220425 15:28:28.084044 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:28:28.161890 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.989\n",
      "I20220425 15:28:28.161921 228398592 solver.cpp:414]     Test net output #1: loss = 0.0299532 (* 1 = 0.0299532 loss)\n",
      "I20220425 15:28:28.187062 228398592 solver.cpp:239] Iteration 9000 (21.8484 iter/s, 4.577s/100 iters), loss = 0.0101963\n",
      "I20220425 15:28:28.187090 228398592 solver.cpp:258]     Train net output #0: loss = 0.010196 (* 1 = 0.010196 loss)\n",
      "I20220425 15:28:28.187096 228398592 sgd_solver.cpp:112] Iteration 9000, lr = 0.00617924\n",
      "I20220425 15:28:30.782379 228398592 solver.cpp:239] Iteration 9100 (38.5356 iter/s, 2.595s/100 iters), loss = 0.00803568\n",
      "I20220425 15:28:30.782407 228398592 solver.cpp:258]     Train net output #0: loss = 0.00803538 (* 1 = 0.00803538 loss)\n",
      "I20220425 15:28:30.782415 228398592 sgd_solver.cpp:112] Iteration 9100, lr = 0.00615496\n",
      "I20220425 15:28:33.360790 228398592 solver.cpp:239] Iteration 9200 (38.7898 iter/s, 2.578s/100 iters), loss = 0.0029423\n",
      "I20220425 15:28:33.360822 228398592 solver.cpp:258]     Train net output #0: loss = 0.00294198 (* 1 = 0.00294198 loss)\n",
      "I20220425 15:28:33.360831 228398592 sgd_solver.cpp:112] Iteration 9200, lr = 0.0061309\n",
      "I20220425 15:28:35.959164 228398592 solver.cpp:239] Iteration 9300 (38.4911 iter/s, 2.598s/100 iters), loss = 0.008839\n",
      "I20220425 15:28:35.959195 228398592 solver.cpp:258]     Train net output #0: loss = 0.00883869 (* 1 = 0.00883869 loss)\n",
      "I20220425 15:28:35.959203 228398592 sgd_solver.cpp:112] Iteration 9300, lr = 0.00610706\n",
      "I20220425 15:28:37.792851 226213888 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:28:38.565279 228398592 solver.cpp:239] Iteration 9400 (38.373 iter/s, 2.606s/100 iters), loss = 0.0211051\n",
      "I20220425 15:28:38.565310 228398592 solver.cpp:258]     Train net output #0: loss = 0.0211048 (* 1 = 0.0211048 loss)\n",
      "I20220425 15:28:38.565317 228398592 sgd_solver.cpp:112] Iteration 9400, lr = 0.00608343\n",
      "I20220425 15:28:41.121162 228398592 solver.cpp:347] Iteration 9500, Testing net (#0)\n",
      "I20220425 15:28:43.004599 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:28:43.083940 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9878\n",
      "I20220425 15:28:43.083967 228398592 solver.cpp:414]     Test net output #1: loss = 0.0337651 (* 1 = 0.0337651 loss)\n",
      "I20220425 15:28:43.109609 228398592 solver.cpp:239] Iteration 9500 (22.007 iter/s, 4.544s/100 iters), loss = 0.00261088\n",
      "I20220425 15:28:43.109638 228398592 solver.cpp:258]     Train net output #0: loss = 0.00261057 (* 1 = 0.00261057 loss)\n",
      "I20220425 15:28:43.109645 228398592 sgd_solver.cpp:112] Iteration 9500, lr = 0.00606002\n",
      "I20220425 15:28:45.827143 228398592 solver.cpp:239] Iteration 9600 (36.8053 iter/s, 2.717s/100 iters), loss = 0.00225291\n",
      "I20220425 15:28:45.827190 228398592 solver.cpp:258]     Train net output #0: loss = 0.00225259 (* 1 = 0.00225259 loss)\n",
      "I20220425 15:28:45.827200 228398592 sgd_solver.cpp:112] Iteration 9600, lr = 0.00603682\n",
      "I20220425 15:28:48.485657 228398592 solver.cpp:239] Iteration 9700 (37.6223 iter/s, 2.658s/100 iters), loss = 0.00456094\n",
      "I20220425 15:28:48.485687 228398592 solver.cpp:258]     Train net output #0: loss = 0.00456062 (* 1 = 0.00456062 loss)\n",
      "I20220425 15:28:48.485694 228398592 sgd_solver.cpp:112] Iteration 9700, lr = 0.00601382\n",
      "I20220425 15:28:51.064574 228398592 solver.cpp:239] Iteration 9800 (38.7898 iter/s, 2.578s/100 iters), loss = 0.0126673\n",
      "I20220425 15:28:51.064604 228398592 solver.cpp:258]     Train net output #0: loss = 0.012667 (* 1 = 0.012667 loss)\n",
      "I20220425 15:28:51.064610 228398592 sgd_solver.cpp:112] Iteration 9800, lr = 0.00599102\n",
      "I20220425 15:28:53.649315 228398592 solver.cpp:239] Iteration 9900 (38.6997 iter/s, 2.584s/100 iters), loss = 0.00607835\n",
      "I20220425 15:28:53.649345 228398592 solver.cpp:258]     Train net output #0: loss = 0.00607803 (* 1 = 0.00607803 loss)\n",
      "I20220425 15:28:53.649353 228398592 sgd_solver.cpp:112] Iteration 9900, lr = 0.00596843\n",
      "I20220425 15:28:56.217202 228398592 solver.cpp:464] Snapshotting to binary proto file mnist/lenet_iter_10000.caffemodel\n",
      "I20220425 15:28:56.227923 228398592 sgd_solver.cpp:284] Snapshotting solver state to binary proto file mnist/lenet_iter_10000.solverstate\n",
      "I20220425 15:28:56.252267 228398592 solver.cpp:327] Iteration 10000, loss = 0.00336127\n",
      "I20220425 15:28:56.252296 228398592 solver.cpp:347] Iteration 10000, Testing net (#0)\n",
      "I20220425 15:28:58.232149 226750464 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 15:28:58.328868 228398592 solver.cpp:414]     Test net output #0: accuracy = 0.9913\n",
      "I20220425 15:28:58.328907 228398592 solver.cpp:414]     Test net output #1: loss = 0.0278027 (* 1 = 0.0278027 loss)\n",
      "I20220425 15:28:58.328913 228398592 solver.cpp:332] Optimization Done.\n",
      "I20220425 15:28:58.328919 228398592 caffe.cpp:250] Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "!cd caffe; bash train_lenet.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3926ab58-468e-4d07-8cc2-4a9219efbaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20220425 12:59:16.467736 229762560 caffe.cpp:275] Use CPU.\n",
      "I20220425 12:59:16.473465 229762560 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I20220425 12:59:16.473490 229762560 net.cpp:53] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "  stage: \"\"\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"mnist/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I20220425 12:59:16.474082 229762560 layer_factory.hpp:77] Creating layer mnist\n",
      "I20220425 12:59:16.475080 229762560 db_lmdb.cpp:35] Opened lmdb mnist/mnist_test_lmdb\n",
      "I20220425 12:59:16.479005 229762560 net.cpp:86] Creating Layer mnist\n",
      "I20220425 12:59:16.479033 229762560 net.cpp:382] mnist -> data\n",
      "I20220425 12:59:16.479053 229762560 net.cpp:382] mnist -> label\n",
      "I20220425 12:59:16.479087 229762560 data_layer.cpp:45] output data size: 100,1,28,28\n",
      "I20220425 12:59:16.480108 229762560 net.cpp:124] Setting up mnist\n",
      "I20220425 12:59:16.480119 229762560 net.cpp:131] Top shape: 100 1 28 28 (78400)\n",
      "I20220425 12:59:16.480127 229762560 net.cpp:131] Top shape: 100 (100)\n",
      "I20220425 12:59:16.480132 229762560 net.cpp:139] Memory required for data: 314000\n",
      "I20220425 12:59:16.480137 229762560 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I20220425 12:59:16.480144 229762560 net.cpp:86] Creating Layer label_mnist_1_split\n",
      "I20220425 12:59:16.480149 229762560 net.cpp:408] label_mnist_1_split <- label\n",
      "I20220425 12:59:16.480155 229762560 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I20220425 12:59:16.480162 229762560 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I20220425 12:59:16.480170 229762560 net.cpp:124] Setting up label_mnist_1_split\n",
      "I20220425 12:59:16.480175 229762560 net.cpp:131] Top shape: 100 (100)\n",
      "I20220425 12:59:16.480180 229762560 net.cpp:131] Top shape: 100 (100)\n",
      "I20220425 12:59:16.480183 229762560 net.cpp:139] Memory required for data: 314800\n",
      "I20220425 12:59:16.480188 229762560 layer_factory.hpp:77] Creating layer conv1\n",
      "I20220425 12:59:16.480208 229762560 net.cpp:86] Creating Layer conv1\n",
      "I20220425 12:59:16.480288 229762560 net.cpp:408] conv1 <- data\n",
      "I20220425 12:59:16.480296 229762560 net.cpp:382] conv1 -> conv1\n",
      "I20220425 12:59:16.480346 229762560 net.cpp:124] Setting up conv1\n",
      "I20220425 12:59:16.480419 229762560 net.cpp:131] Top shape: 100 20 24 24 (1152000)\n",
      "I20220425 12:59:16.480427 229762560 net.cpp:139] Memory required for data: 4922800\n",
      "I20220425 12:59:16.480440 229762560 layer_factory.hpp:77] Creating layer pool1\n",
      "I20220425 12:59:16.480446 229762560 net.cpp:86] Creating Layer pool1\n",
      "I20220425 12:59:16.480451 229762560 net.cpp:408] pool1 <- conv1\n",
      "I20220425 12:59:16.480456 229762560 net.cpp:382] pool1 -> pool1\n",
      "I20220425 12:59:16.480471 229762560 net.cpp:124] Setting up pool1\n",
      "I20220425 12:59:16.480476 229762560 net.cpp:131] Top shape: 100 20 12 12 (288000)\n",
      "I20220425 12:59:16.480481 229762560 net.cpp:139] Memory required for data: 6074800\n",
      "I20220425 12:59:16.480486 229762560 layer_factory.hpp:77] Creating layer conv2\n",
      "I20220425 12:59:16.480494 229762560 net.cpp:86] Creating Layer conv2\n",
      "I20220425 12:59:16.480497 229762560 net.cpp:408] conv2 <- pool1\n",
      "I20220425 12:59:16.480504 229762560 net.cpp:382] conv2 -> conv2\n",
      "I20220425 12:59:16.480731 229762560 net.cpp:124] Setting up conv2\n",
      "I20220425 12:59:16.480737 229762560 net.cpp:131] Top shape: 100 50 8 8 (320000)\n",
      "I20220425 12:59:16.480743 229762560 net.cpp:139] Memory required for data: 7354800\n",
      "I20220425 12:59:16.480751 229762560 layer_factory.hpp:77] Creating layer pool2\n",
      "I20220425 12:59:16.480756 229762560 net.cpp:86] Creating Layer pool2\n",
      "I20220425 12:59:16.480760 229762560 net.cpp:408] pool2 <- conv2\n",
      "I20220425 12:59:16.480765 229762560 net.cpp:382] pool2 -> pool2\n",
      "I20220425 12:59:16.480772 229762560 net.cpp:124] Setting up pool2\n",
      "I20220425 12:59:16.480777 229762560 net.cpp:131] Top shape: 100 50 4 4 (80000)\n",
      "I20220425 12:59:16.480782 229762560 net.cpp:139] Memory required for data: 7674800\n",
      "I20220425 12:59:16.480787 229762560 layer_factory.hpp:77] Creating layer ip1\n",
      "I20220425 12:59:16.480795 229762560 net.cpp:86] Creating Layer ip1\n",
      "I20220425 12:59:16.480800 229762560 net.cpp:408] ip1 <- pool2\n",
      "I20220425 12:59:16.480806 229762560 net.cpp:382] ip1 -> ip1\n",
      "I20220425 12:59:16.483779 229762560 net.cpp:124] Setting up ip1\n",
      "I20220425 12:59:16.483793 229762560 net.cpp:131] Top shape: 100 500 (50000)\n",
      "I20220425 12:59:16.483798 229762560 net.cpp:139] Memory required for data: 7874800\n",
      "I20220425 12:59:16.483803 229762560 layer_factory.hpp:77] Creating layer relu1\n",
      "I20220425 12:59:16.483814 229762560 net.cpp:86] Creating Layer relu1\n",
      "I20220425 12:59:16.483817 229762560 net.cpp:408] relu1 <- ip1\n",
      "I20220425 12:59:16.483827 229762560 net.cpp:369] relu1 -> ip1 (in-place)\n",
      "I20220425 12:59:16.483834 229762560 net.cpp:124] Setting up relu1\n",
      "I20220425 12:59:16.483837 229762560 net.cpp:131] Top shape: 100 500 (50000)\n",
      "I20220425 12:59:16.483842 229762560 net.cpp:139] Memory required for data: 8074800\n",
      "I20220425 12:59:16.483845 229762560 layer_factory.hpp:77] Creating layer ip2\n",
      "I20220425 12:59:16.483851 229762560 net.cpp:86] Creating Layer ip2\n",
      "I20220425 12:59:16.483855 229762560 net.cpp:408] ip2 <- ip1\n",
      "I20220425 12:59:16.483860 229762560 net.cpp:382] ip2 -> ip2\n",
      "I20220425 12:59:16.483899 229762560 net.cpp:124] Setting up ip2\n",
      "I20220425 12:59:16.483903 229762560 net.cpp:131] Top shape: 100 10 (1000)\n",
      "I20220425 12:59:16.483907 229762560 net.cpp:139] Memory required for data: 8078800\n",
      "I20220425 12:59:16.483912 229762560 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I20220425 12:59:16.483917 229762560 net.cpp:86] Creating Layer ip2_ip2_0_split\n",
      "I20220425 12:59:16.483986 229762560 net.cpp:408] ip2_ip2_0_split <- ip2\n",
      "I20220425 12:59:16.483999 229762560 net.cpp:382] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I20220425 12:59:16.484006 229762560 net.cpp:382] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I20220425 12:59:16.484014 229762560 net.cpp:124] Setting up ip2_ip2_0_split\n",
      "I20220425 12:59:16.484019 229762560 net.cpp:131] Top shape: 100 10 (1000)\n",
      "I20220425 12:59:16.484022 229762560 net.cpp:131] Top shape: 100 10 (1000)\n",
      "I20220425 12:59:16.484027 229762560 net.cpp:139] Memory required for data: 8086800\n",
      "I20220425 12:59:16.484031 229762560 layer_factory.hpp:77] Creating layer accuracy\n",
      "I20220425 12:59:16.484036 229762560 net.cpp:86] Creating Layer accuracy\n",
      "I20220425 12:59:16.484040 229762560 net.cpp:408] accuracy <- ip2_ip2_0_split_0\n",
      "I20220425 12:59:16.484112 229762560 net.cpp:408] accuracy <- label_mnist_1_split_0\n",
      "I20220425 12:59:16.484118 229762560 net.cpp:382] accuracy -> accuracy\n",
      "I20220425 12:59:16.484129 229762560 net.cpp:124] Setting up accuracy\n",
      "I20220425 12:59:16.484175 229762560 net.cpp:131] Top shape: (1)\n",
      "I20220425 12:59:16.484182 229762560 net.cpp:139] Memory required for data: 8086804\n",
      "I20220425 12:59:16.484186 229762560 layer_factory.hpp:77] Creating layer loss\n",
      "I20220425 12:59:16.484191 229762560 net.cpp:86] Creating Layer loss\n",
      "I20220425 12:59:16.484195 229762560 net.cpp:408] loss <- ip2_ip2_0_split_1\n",
      "I20220425 12:59:16.484200 229762560 net.cpp:408] loss <- label_mnist_1_split_1\n",
      "I20220425 12:59:16.484205 229762560 net.cpp:382] loss -> loss\n",
      "I20220425 12:59:16.484215 229762560 layer_factory.hpp:77] Creating layer loss\n",
      "I20220425 12:59:16.484225 229762560 net.cpp:124] Setting up loss\n",
      "I20220425 12:59:16.484230 229762560 net.cpp:131] Top shape: (1)\n",
      "I20220425 12:59:16.484233 229762560 net.cpp:134]     with loss weight 1\n",
      "I20220425 12:59:16.484238 229762560 net.cpp:139] Memory required for data: 8086808\n",
      "I20220425 12:59:16.484242 229762560 net.cpp:200] loss needs backward computation.\n",
      "I20220425 12:59:16.484333 229762560 net.cpp:202] accuracy does not need backward computation.\n",
      "I20220425 12:59:16.484339 229762560 net.cpp:200] ip2_ip2_0_split needs backward computation.\n",
      "I20220425 12:59:16.484344 229762560 net.cpp:200] ip2 needs backward computation.\n",
      "I20220425 12:59:16.484347 229762560 net.cpp:200] relu1 needs backward computation.\n",
      "I20220425 12:59:16.484351 229762560 net.cpp:200] ip1 needs backward computation.\n",
      "I20220425 12:59:16.484355 229762560 net.cpp:200] pool2 needs backward computation.\n",
      "I20220425 12:59:16.484359 229762560 net.cpp:200] conv2 needs backward computation.\n",
      "I20220425 12:59:16.484362 229762560 net.cpp:200] pool1 needs backward computation.\n",
      "I20220425 12:59:16.484366 229762560 net.cpp:200] conv1 needs backward computation.\n",
      "I20220425 12:59:16.484370 229762560 net.cpp:202] label_mnist_1_split does not need backward computation.\n",
      "I20220425 12:59:16.484380 229762560 net.cpp:202] mnist does not need backward computation.\n",
      "I20220425 12:59:16.484385 229762560 net.cpp:244] This network produces output accuracy\n",
      "I20220425 12:59:16.484483 229762560 net.cpp:244] This network produces output loss\n",
      "I20220425 12:59:16.484491 229762560 net.cpp:257] Network initialization done.\n",
      "[libprotobuf ERROR google/protobuf/io/coded_stream.cc:193] A protocol message was rejected because it was too big (more than 0 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in third_party/protobuf/src/google/protobuf/io/coded_stream.h.\n",
      "I20220425 12:59:16.487980 229762560 caffe.cpp:281] Running for 100 iterations.\n",
      "I20220425 12:59:16.515950 229762560 caffe.cpp:304] Batch 0, accuracy = 0.06\n",
      "I20220425 12:59:16.515980 229762560 caffe.cpp:304] Batch 0, loss = 2.38991\n",
      "I20220425 12:59:16.538208 229762560 caffe.cpp:304] Batch 1, accuracy = 0.09\n",
      "I20220425 12:59:16.538239 229762560 caffe.cpp:304] Batch 1, loss = 2.35619\n",
      "I20220425 12:59:16.560204 229762560 caffe.cpp:304] Batch 2, accuracy = 0.14\n",
      "I20220425 12:59:16.560227 229762560 caffe.cpp:304] Batch 2, loss = 2.33937\n",
      "I20220425 12:59:16.581331 229762560 caffe.cpp:304] Batch 3, accuracy = 0.07\n",
      "I20220425 12:59:16.581362 229762560 caffe.cpp:304] Batch 3, loss = 2.37234\n",
      "I20220425 12:59:16.603658 229762560 caffe.cpp:304] Batch 4, accuracy = 0.12\n",
      "I20220425 12:59:16.603690 229762560 caffe.cpp:304] Batch 4, loss = 2.32986\n",
      "I20220425 12:59:16.628226 229762560 caffe.cpp:304] Batch 5, accuracy = 0.08\n",
      "I20220425 12:59:16.628262 229762560 caffe.cpp:304] Batch 5, loss = 2.31878\n",
      "I20220425 12:59:16.653673 229762560 caffe.cpp:304] Batch 6, accuracy = 0.09\n",
      "I20220425 12:59:16.653707 229762560 caffe.cpp:304] Batch 6, loss = 2.33214\n",
      "I20220425 12:59:16.678301 229762560 caffe.cpp:304] Batch 7, accuracy = 0.06\n",
      "I20220425 12:59:16.678336 229762560 caffe.cpp:304] Batch 7, loss = 2.38883\n",
      "I20220425 12:59:16.702505 229762560 caffe.cpp:304] Batch 8, accuracy = 0.05\n",
      "I20220425 12:59:16.702535 229762560 caffe.cpp:304] Batch 8, loss = 2.37857\n",
      "I20220425 12:59:16.723933 229762560 caffe.cpp:304] Batch 9, accuracy = 0.06\n",
      "I20220425 12:59:16.723963 229762560 caffe.cpp:304] Batch 9, loss = 2.35777\n",
      "I20220425 12:59:16.745795 229762560 caffe.cpp:304] Batch 10, accuracy = 0.08\n",
      "I20220425 12:59:16.745823 229762560 caffe.cpp:304] Batch 10, loss = 2.34659\n",
      "I20220425 12:59:16.765120 229762560 caffe.cpp:304] Batch 11, accuracy = 0.05\n",
      "I20220425 12:59:16.765146 229762560 caffe.cpp:304] Batch 11, loss = 2.33405\n",
      "I20220425 12:59:16.786531 229762560 caffe.cpp:304] Batch 12, accuracy = 0.04\n",
      "I20220425 12:59:16.786563 229762560 caffe.cpp:304] Batch 12, loss = 2.3933\n",
      "I20220425 12:59:16.806898 229762560 caffe.cpp:304] Batch 13, accuracy = 0.1\n",
      "I20220425 12:59:16.806926 229762560 caffe.cpp:304] Batch 13, loss = 2.33444\n",
      "I20220425 12:59:16.827395 229762560 caffe.cpp:304] Batch 14, accuracy = 0.1\n",
      "I20220425 12:59:16.827426 229762560 caffe.cpp:304] Batch 14, loss = 2.32396\n",
      "I20220425 12:59:16.847687 229762560 caffe.cpp:304] Batch 15, accuracy = 0.1\n",
      "I20220425 12:59:16.847716 229762560 caffe.cpp:304] Batch 15, loss = 2.32431\n",
      "I20220425 12:59:16.867955 229762560 caffe.cpp:304] Batch 16, accuracy = 0.13\n",
      "I20220425 12:59:16.867981 229762560 caffe.cpp:304] Batch 16, loss = 2.29506\n",
      "I20220425 12:59:16.888936 229762560 caffe.cpp:304] Batch 17, accuracy = 0.1\n",
      "I20220425 12:59:16.888964 229762560 caffe.cpp:304] Batch 17, loss = 2.40145\n",
      "I20220425 12:59:16.909077 229762560 caffe.cpp:304] Batch 18, accuracy = 0.12\n",
      "I20220425 12:59:16.909107 229762560 caffe.cpp:304] Batch 18, loss = 2.34123\n",
      "I20220425 12:59:16.929455 229762560 caffe.cpp:304] Batch 19, accuracy = 0.07\n",
      "I20220425 12:59:16.929539 229762560 caffe.cpp:304] Batch 19, loss = 2.36138\n",
      "I20220425 12:59:16.950330 229762560 caffe.cpp:304] Batch 20, accuracy = 0.06\n",
      "I20220425 12:59:16.950362 229762560 caffe.cpp:304] Batch 20, loss = 2.33384\n",
      "I20220425 12:59:16.971977 229762560 caffe.cpp:304] Batch 21, accuracy = 0.07\n",
      "I20220425 12:59:16.972005 229762560 caffe.cpp:304] Batch 21, loss = 2.33383\n",
      "I20220425 12:59:16.992292 229762560 caffe.cpp:304] Batch 22, accuracy = 0.12\n",
      "I20220425 12:59:16.992321 229762560 caffe.cpp:304] Batch 22, loss = 2.31593\n",
      "I20220425 12:59:17.013547 229762560 caffe.cpp:304] Batch 23, accuracy = 0.08\n",
      "I20220425 12:59:17.013577 229762560 caffe.cpp:304] Batch 23, loss = 2.373\n",
      "I20220425 12:59:17.034714 229762560 caffe.cpp:304] Batch 24, accuracy = 0.08\n",
      "I20220425 12:59:17.034744 229762560 caffe.cpp:304] Batch 24, loss = 2.38455\n",
      "I20220425 12:59:17.057689 229762560 caffe.cpp:304] Batch 25, accuracy = 0.13\n",
      "I20220425 12:59:17.057721 229762560 caffe.cpp:304] Batch 25, loss = 2.30541\n",
      "I20220425 12:59:17.079918 229762560 caffe.cpp:304] Batch 26, accuracy = 0.05\n",
      "I20220425 12:59:17.079946 229762560 caffe.cpp:304] Batch 26, loss = 2.34846\n",
      "I20220425 12:59:17.102203 229762560 caffe.cpp:304] Batch 27, accuracy = 0.06\n",
      "I20220425 12:59:17.102234 229762560 caffe.cpp:304] Batch 27, loss = 2.41844\n",
      "I20220425 12:59:17.122925 229762560 caffe.cpp:304] Batch 28, accuracy = 0.09\n",
      "I20220425 12:59:17.122952 229762560 caffe.cpp:304] Batch 28, loss = 2.30792\n",
      "I20220425 12:59:17.143923 229762560 caffe.cpp:304] Batch 29, accuracy = 0.12\n",
      "I20220425 12:59:17.143945 229762560 caffe.cpp:304] Batch 29, loss = 2.29473\n",
      "I20220425 12:59:17.164631 229762560 caffe.cpp:304] Batch 30, accuracy = 0.07\n",
      "I20220425 12:59:17.164657 229762560 caffe.cpp:304] Batch 30, loss = 2.35542\n",
      "I20220425 12:59:17.187098 229762560 caffe.cpp:304] Batch 31, accuracy = 0.09\n",
      "I20220425 12:59:17.187125 229762560 caffe.cpp:304] Batch 31, loss = 2.3347\n",
      "I20220425 12:59:17.208988 229762560 caffe.cpp:304] Batch 32, accuracy = 0.05\n",
      "I20220425 12:59:17.209017 229762560 caffe.cpp:304] Batch 32, loss = 2.39728\n",
      "I20220425 12:59:17.231500 229762560 caffe.cpp:304] Batch 33, accuracy = 0.05\n",
      "I20220425 12:59:17.231530 229762560 caffe.cpp:304] Batch 33, loss = 2.40485\n",
      "I20220425 12:59:17.255084 229762560 caffe.cpp:304] Batch 34, accuracy = 0.1\n",
      "I20220425 12:59:17.255121 229762560 caffe.cpp:304] Batch 34, loss = 2.3353\n",
      "I20220425 12:59:17.278709 229762560 caffe.cpp:304] Batch 35, accuracy = 0.08\n",
      "I20220425 12:59:17.278744 229762560 caffe.cpp:304] Batch 35, loss = 2.32552\n",
      "I20220425 12:59:17.302876 229762560 caffe.cpp:304] Batch 36, accuracy = 0.06\n",
      "I20220425 12:59:17.302915 229762560 caffe.cpp:304] Batch 36, loss = 2.39539\n",
      "I20220425 12:59:17.328433 229762560 caffe.cpp:304] Batch 37, accuracy = 0.13\n",
      "I20220425 12:59:17.328554 229762560 caffe.cpp:304] Batch 37, loss = 2.33384\n",
      "I20220425 12:59:17.353363 229762560 caffe.cpp:304] Batch 38, accuracy = 0.15\n",
      "I20220425 12:59:17.353396 229762560 caffe.cpp:304] Batch 38, loss = 2.31721\n",
      "I20220425 12:59:17.378571 229762560 caffe.cpp:304] Batch 39, accuracy = 0.08\n",
      "I20220425 12:59:17.378605 229762560 caffe.cpp:304] Batch 39, loss = 2.34842\n",
      "I20220425 12:59:17.403008 229762560 caffe.cpp:304] Batch 40, accuracy = 0.1\n",
      "I20220425 12:59:17.403070 229762560 caffe.cpp:304] Batch 40, loss = 2.37345\n",
      "I20220425 12:59:17.426286 229762560 caffe.cpp:304] Batch 41, accuracy = 0.13\n",
      "I20220425 12:59:17.426326 229762560 caffe.cpp:304] Batch 41, loss = 2.33595\n",
      "I20220425 12:59:17.449200 229762560 caffe.cpp:304] Batch 42, accuracy = 0.12\n",
      "I20220425 12:59:17.449230 229762560 caffe.cpp:304] Batch 42, loss = 2.28668\n",
      "I20220425 12:59:17.471840 229762560 caffe.cpp:304] Batch 43, accuracy = 0.12\n",
      "I20220425 12:59:17.471870 229762560 caffe.cpp:304] Batch 43, loss = 2.35408\n",
      "I20220425 12:59:17.493809 229762560 caffe.cpp:304] Batch 44, accuracy = 0.12\n",
      "I20220425 12:59:17.493837 229762560 caffe.cpp:304] Batch 44, loss = 2.33974\n",
      "I20220425 12:59:17.516273 229762560 caffe.cpp:304] Batch 45, accuracy = 0.09\n",
      "I20220425 12:59:17.516300 229762560 caffe.cpp:304] Batch 45, loss = 2.29028\n",
      "I20220425 12:59:17.538152 229762560 caffe.cpp:304] Batch 46, accuracy = 0.12\n",
      "I20220425 12:59:17.538182 229762560 caffe.cpp:304] Batch 46, loss = 2.33908\n",
      "I20220425 12:59:17.559969 229762560 caffe.cpp:304] Batch 47, accuracy = 0.13\n",
      "I20220425 12:59:17.560000 229762560 caffe.cpp:304] Batch 47, loss = 2.28437\n",
      "I20220425 12:59:17.582417 229762560 caffe.cpp:304] Batch 48, accuracy = 0.09\n",
      "I20220425 12:59:17.582453 229762560 caffe.cpp:304] Batch 48, loss = 2.35456\n",
      "I20220425 12:59:17.604614 229762560 caffe.cpp:304] Batch 49, accuracy = 0.14\n",
      "I20220425 12:59:17.604650 229762560 caffe.cpp:304] Batch 49, loss = 2.35439\n",
      "I20220425 12:59:17.627140 229762560 caffe.cpp:304] Batch 50, accuracy = 0.13\n",
      "I20220425 12:59:17.627171 229762560 caffe.cpp:304] Batch 50, loss = 2.32893\n",
      "I20220425 12:59:17.647930 229762560 caffe.cpp:304] Batch 51, accuracy = 0.05\n",
      "I20220425 12:59:17.647958 229762560 caffe.cpp:304] Batch 51, loss = 2.36554\n",
      "I20220425 12:59:17.669291 229762560 caffe.cpp:304] Batch 52, accuracy = 0.06\n",
      "I20220425 12:59:17.669342 229762560 caffe.cpp:304] Batch 52, loss = 2.37779\n",
      "I20220425 12:59:17.690728 229762560 caffe.cpp:304] Batch 53, accuracy = 0.09\n",
      "I20220425 12:59:17.690757 229762560 caffe.cpp:304] Batch 53, loss = 2.32961\n",
      "I20220425 12:59:17.711444 229762560 caffe.cpp:304] Batch 54, accuracy = 0.04\n",
      "I20220425 12:59:17.711472 229762560 caffe.cpp:304] Batch 54, loss = 2.42018\n",
      "I20220425 12:59:17.732456 229762560 caffe.cpp:304] Batch 55, accuracy = 0.08\n",
      "I20220425 12:59:17.732483 229762560 caffe.cpp:304] Batch 55, loss = 2.36644\n",
      "I20220425 12:59:17.752785 229762560 caffe.cpp:304] Batch 56, accuracy = 0.03\n",
      "I20220425 12:59:17.752815 229762560 caffe.cpp:304] Batch 56, loss = 2.36298\n",
      "I20220425 12:59:17.774237 229762560 caffe.cpp:304] Batch 57, accuracy = 0.06\n",
      "I20220425 12:59:17.774263 229762560 caffe.cpp:304] Batch 57, loss = 2.30648\n",
      "I20220425 12:59:17.794294 229762560 caffe.cpp:304] Batch 58, accuracy = 0.09\n",
      "I20220425 12:59:17.794337 229762560 caffe.cpp:304] Batch 58, loss = 2.34343\n",
      "I20220425 12:59:17.816882 229762560 caffe.cpp:304] Batch 59, accuracy = 0.06\n",
      "I20220425 12:59:17.816906 229762560 caffe.cpp:304] Batch 59, loss = 2.37296\n",
      "I20220425 12:59:17.839702 229762560 caffe.cpp:304] Batch 60, accuracy = 0.03\n",
      "I20220425 12:59:17.839730 229762560 caffe.cpp:304] Batch 60, loss = 2.3376\n",
      "I20220425 12:59:17.861953 229762560 caffe.cpp:304] Batch 61, accuracy = 0.04\n",
      "I20220425 12:59:17.861984 229762560 caffe.cpp:304] Batch 61, loss = 2.37126\n",
      "I20220425 12:59:17.884089 229762560 caffe.cpp:304] Batch 62, accuracy = 0.08\n",
      "I20220425 12:59:17.884120 229762560 caffe.cpp:304] Batch 62, loss = 2.33514\n",
      "I20220425 12:59:17.904479 229762560 caffe.cpp:304] Batch 63, accuracy = 0.03\n",
      "I20220425 12:59:17.904511 229762560 caffe.cpp:304] Batch 63, loss = 2.32293\n",
      "I20220425 12:59:17.927752 229762560 caffe.cpp:304] Batch 64, accuracy = 0.09\n",
      "I20220425 12:59:17.927866 229762560 caffe.cpp:304] Batch 64, loss = 2.35341\n",
      "I20220425 12:59:17.949718 229762560 caffe.cpp:304] Batch 65, accuracy = 0.11\n",
      "I20220425 12:59:17.949749 229762560 caffe.cpp:304] Batch 65, loss = 2.34886\n",
      "I20220425 12:59:17.970722 229762560 caffe.cpp:304] Batch 66, accuracy = 0.08\n",
      "I20220425 12:59:17.970744 229762560 caffe.cpp:304] Batch 66, loss = 2.30971\n",
      "I20220425 12:59:17.992532 229762560 caffe.cpp:304] Batch 67, accuracy = 0.07\n",
      "I20220425 12:59:17.992560 229762560 caffe.cpp:304] Batch 67, loss = 2.40417\n",
      "I20220425 12:59:18.014048 229762560 caffe.cpp:304] Batch 68, accuracy = 0.02\n",
      "I20220425 12:59:18.014077 229762560 caffe.cpp:304] Batch 68, loss = 2.36365\n",
      "I20220425 12:59:18.038708 229762560 caffe.cpp:304] Batch 69, accuracy = 0.02\n",
      "I20220425 12:59:18.038743 229762560 caffe.cpp:304] Batch 69, loss = 2.39148\n",
      "I20220425 12:59:18.063305 229762560 caffe.cpp:304] Batch 70, accuracy = 0.04\n",
      "I20220425 12:59:18.063339 229762560 caffe.cpp:304] Batch 70, loss = 2.32037\n",
      "I20220425 12:59:18.088222 229762560 caffe.cpp:304] Batch 71, accuracy = 0.06\n",
      "I20220425 12:59:18.088260 229762560 caffe.cpp:304] Batch 71, loss = 2.2868\n",
      "I20220425 12:59:18.112430 229762560 caffe.cpp:304] Batch 72, accuracy = 0.1\n",
      "I20220425 12:59:18.112462 229762560 caffe.cpp:304] Batch 72, loss = 2.31037\n",
      "I20220425 12:59:18.135002 229762560 caffe.cpp:304] Batch 73, accuracy = 0.07\n",
      "I20220425 12:59:18.135032 229762560 caffe.cpp:304] Batch 73, loss = 2.35351\n",
      "I20220425 12:59:18.156927 229762560 caffe.cpp:304] Batch 74, accuracy = 0.03\n",
      "I20220425 12:59:18.156958 229762560 caffe.cpp:304] Batch 74, loss = 2.33972\n",
      "I20220425 12:59:18.176633 229762560 caffe.cpp:304] Batch 75, accuracy = 0.04\n",
      "I20220425 12:59:18.176659 229762560 caffe.cpp:304] Batch 75, loss = 2.36216\n",
      "I20220425 12:59:18.196611 229762560 caffe.cpp:304] Batch 76, accuracy = 0.09\n",
      "I20220425 12:59:18.196640 229762560 caffe.cpp:304] Batch 76, loss = 2.35083\n",
      "I20220425 12:59:18.217072 229762560 caffe.cpp:304] Batch 77, accuracy = 0.07\n",
      "I20220425 12:59:18.217095 229762560 caffe.cpp:304] Batch 77, loss = 2.36563\n",
      "I20220425 12:59:18.238902 229762560 caffe.cpp:304] Batch 78, accuracy = 0.07\n",
      "I20220425 12:59:18.238932 229762560 caffe.cpp:304] Batch 78, loss = 2.43916\n",
      "I20220425 12:59:18.258847 229762560 caffe.cpp:304] Batch 79, accuracy = 0.02\n",
      "I20220425 12:59:18.258878 229762560 caffe.cpp:304] Batch 79, loss = 2.42223\n",
      "I20220425 12:59:18.279592 229762560 caffe.cpp:304] Batch 80, accuracy = 0.05\n",
      "I20220425 12:59:18.279621 229762560 caffe.cpp:304] Batch 80, loss = 2.442\n",
      "I20220425 12:59:18.300287 229762560 caffe.cpp:304] Batch 81, accuracy = 0.05\n",
      "I20220425 12:59:18.300315 229762560 caffe.cpp:304] Batch 81, loss = 2.38668\n",
      "I20220425 12:59:18.320673 229762560 caffe.cpp:304] Batch 82, accuracy = 0.09\n",
      "I20220425 12:59:18.320700 229762560 caffe.cpp:304] Batch 82, loss = 2.3118\n",
      "I20220425 12:59:18.341747 229762560 caffe.cpp:304] Batch 83, accuracy = 0.08\n",
      "I20220425 12:59:18.341780 229762560 caffe.cpp:304] Batch 83, loss = 2.32456\n",
      "I20220425 12:59:18.362532 229762560 caffe.cpp:304] Batch 84, accuracy = 0.09\n",
      "I20220425 12:59:18.362560 229762560 caffe.cpp:304] Batch 84, loss = 2.3251\n",
      "I20220425 12:59:18.382658 229762560 caffe.cpp:304] Batch 85, accuracy = 0.06\n",
      "I20220425 12:59:18.382689 229762560 caffe.cpp:304] Batch 85, loss = 2.38749\n",
      "I20220425 12:59:18.403887 229762560 caffe.cpp:304] Batch 86, accuracy = 0.06\n",
      "I20220425 12:59:18.403913 229762560 caffe.cpp:304] Batch 86, loss = 2.35087\n",
      "I20220425 12:59:18.424870 229762560 caffe.cpp:304] Batch 87, accuracy = 0.12\n",
      "I20220425 12:59:18.424901 229762560 caffe.cpp:304] Batch 87, loss = 2.32492\n",
      "I20220425 12:59:18.445869 229762560 caffe.cpp:304] Batch 88, accuracy = 0.05\n",
      "I20220425 12:59:18.445899 229762560 caffe.cpp:304] Batch 88, loss = 2.32371\n",
      "I20220425 12:59:18.466176 229762560 caffe.cpp:304] Batch 89, accuracy = 0.04\n",
      "I20220425 12:59:18.466207 229762560 caffe.cpp:304] Batch 89, loss = 2.31733\n",
      "I20220425 12:59:18.488751 229762560 caffe.cpp:304] Batch 90, accuracy = 0.05\n",
      "I20220425 12:59:18.488780 229762560 caffe.cpp:304] Batch 90, loss = 2.33704\n",
      "I20220425 12:59:18.510210 229762560 caffe.cpp:304] Batch 91, accuracy = 0.06\n",
      "I20220425 12:59:18.510241 229762560 caffe.cpp:304] Batch 91, loss = 2.36003\n",
      "I20220425 12:59:18.531652 229762560 caffe.cpp:304] Batch 92, accuracy = 0.09\n",
      "I20220425 12:59:18.531680 229762560 caffe.cpp:304] Batch 92, loss = 2.35529\n",
      "I20220425 12:59:18.556635 229762560 caffe.cpp:304] Batch 93, accuracy = 0.03\n",
      "I20220425 12:59:18.556669 229762560 caffe.cpp:304] Batch 93, loss = 2.39153\n",
      "I20220425 12:59:18.581738 229762560 caffe.cpp:304] Batch 94, accuracy = 0.08\n",
      "I20220425 12:59:18.581777 229762560 caffe.cpp:304] Batch 94, loss = 2.36956\n",
      "I20220425 12:59:18.606513 229762560 caffe.cpp:304] Batch 95, accuracy = 0.04\n",
      "I20220425 12:59:18.606545 229762560 caffe.cpp:304] Batch 95, loss = 2.31466\n",
      "I20220425 12:59:18.607064 49074176 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I20220425 12:59:18.630587 229762560 caffe.cpp:304] Batch 96, accuracy = 0.05\n",
      "I20220425 12:59:18.630620 229762560 caffe.cpp:304] Batch 96, loss = 2.36782\n",
      "I20220425 12:59:18.652614 229762560 caffe.cpp:304] Batch 97, accuracy = 0.08\n",
      "I20220425 12:59:18.652657 229762560 caffe.cpp:304] Batch 97, loss = 2.37038\n",
      "I20220425 12:59:18.674191 229762560 caffe.cpp:304] Batch 98, accuracy = 0.07\n",
      "I20220425 12:59:18.674221 229762560 caffe.cpp:304] Batch 98, loss = 2.4025\n",
      "I20220425 12:59:18.693629 229762560 caffe.cpp:304] Batch 99, accuracy = 0.08\n",
      "I20220425 12:59:18.693656 229762560 caffe.cpp:304] Batch 99, loss = 2.37604\n",
      "I20220425 12:59:18.693661 229762560 caffe.cpp:309] Loss: 2.35099\n",
      "I20220425 12:59:18.693671 229762560 caffe.cpp:321] accuracy = 0.0776\n",
      "I20220425 12:59:18.693677 229762560 caffe.cpp:321] loss = 2.35099 (* 1 = 2.35099 loss)\n"
     ]
    }
   ],
   "source": [
    "!cd caffe;bash test_lenet.sh -iterations 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebd2d729-a239-411a-a533-4c16ae27eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20220425 13:10:08.976980 240875008 caffe.cpp:343] Use CPU.\n",
      "I20220425 13:10:08.979107 240875008 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist\n",
      "I20220425 13:10:08.979125 240875008 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I20220425 13:10:08.979131 240875008 net.cpp:53] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "  level: 0\n",
      "  stage: \"\"\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"mnist/mnist_train_lmdb\"\n",
      "    batch_size: 64\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I20220425 13:10:08.979589 240875008 layer_factory.hpp:77] Creating layer mnist\n",
      "I20220425 13:10:08.981699 240875008 db_lmdb.cpp:35] Opened lmdb mnist/mnist_train_lmdb\n",
      "I20220425 13:10:08.985749 240875008 net.cpp:86] Creating Layer mnist\n",
      "I20220425 13:10:08.985780 240875008 net.cpp:382] mnist -> data\n",
      "I20220425 13:10:08.985797 240875008 net.cpp:382] mnist -> label\n",
      "I20220425 13:10:08.985831 240875008 data_layer.cpp:45] output data size: 64,1,28,28\n",
      "I20220425 13:10:08.986588 240875008 net.cpp:124] Setting up mnist\n",
      "I20220425 13:10:08.986598 240875008 net.cpp:131] Top shape: 64 1 28 28 (50176)\n",
      "I20220425 13:10:08.986604 240875008 net.cpp:131] Top shape: 64 (64)\n",
      "I20220425 13:10:08.986609 240875008 net.cpp:139] Memory required for data: 200960\n",
      "I20220425 13:10:08.986614 240875008 layer_factory.hpp:77] Creating layer conv1\n",
      "I20220425 13:10:08.986634 240875008 net.cpp:86] Creating Layer conv1\n",
      "I20220425 13:10:08.986639 240875008 net.cpp:408] conv1 <- data\n",
      "I20220425 13:10:08.986644 240875008 net.cpp:382] conv1 -> conv1\n",
      "I20220425 13:10:08.986690 240875008 net.cpp:124] Setting up conv1\n",
      "I20220425 13:10:08.986696 240875008 net.cpp:131] Top shape: 64 20 24 24 (737280)\n",
      "I20220425 13:10:08.986701 240875008 net.cpp:139] Memory required for data: 3150080\n",
      "I20220425 13:10:08.986711 240875008 layer_factory.hpp:77] Creating layer pool1\n",
      "I20220425 13:10:08.986717 240875008 net.cpp:86] Creating Layer pool1\n",
      "I20220425 13:10:08.986722 240875008 net.cpp:408] pool1 <- conv1\n",
      "I20220425 13:10:08.986726 240875008 net.cpp:382] pool1 -> pool1\n",
      "I20220425 13:10:08.986738 240875008 net.cpp:124] Setting up pool1\n",
      "I20220425 13:10:08.986742 240875008 net.cpp:131] Top shape: 64 20 12 12 (184320)\n",
      "I20220425 13:10:08.986747 240875008 net.cpp:139] Memory required for data: 3887360\n",
      "I20220425 13:10:08.986752 240875008 layer_factory.hpp:77] Creating layer conv2\n",
      "I20220425 13:10:08.986853 240875008 net.cpp:86] Creating Layer conv2\n",
      "I20220425 13:10:08.986860 240875008 net.cpp:408] conv2 <- pool1\n",
      "I20220425 13:10:08.986865 240875008 net.cpp:382] conv2 -> conv2\n",
      "I20220425 13:10:08.987097 240875008 net.cpp:124] Setting up conv2\n",
      "I20220425 13:10:08.987104 240875008 net.cpp:131] Top shape: 64 50 8 8 (204800)\n",
      "I20220425 13:10:08.987110 240875008 net.cpp:139] Memory required for data: 4706560\n",
      "I20220425 13:10:08.987118 240875008 layer_factory.hpp:77] Creating layer pool2\n",
      "I20220425 13:10:08.987123 240875008 net.cpp:86] Creating Layer pool2\n",
      "I20220425 13:10:08.987128 240875008 net.cpp:408] pool2 <- conv2\n",
      "I20220425 13:10:08.987133 240875008 net.cpp:382] pool2 -> pool2\n",
      "I20220425 13:10:08.987140 240875008 net.cpp:124] Setting up pool2\n",
      "I20220425 13:10:08.987144 240875008 net.cpp:131] Top shape: 64 50 4 4 (51200)\n",
      "I20220425 13:10:08.987149 240875008 net.cpp:139] Memory required for data: 4911360\n",
      "I20220425 13:10:08.987154 240875008 layer_factory.hpp:77] Creating layer ip1\n",
      "I20220425 13:10:08.987162 240875008 net.cpp:86] Creating Layer ip1\n",
      "I20220425 13:10:08.987166 240875008 net.cpp:408] ip1 <- pool2\n",
      "I20220425 13:10:08.987171 240875008 net.cpp:382] ip1 -> ip1\n",
      "I20220425 13:10:08.990271 240875008 net.cpp:124] Setting up ip1\n",
      "I20220425 13:10:08.990290 240875008 net.cpp:131] Top shape: 64 500 (32000)\n",
      "I20220425 13:10:08.990298 240875008 net.cpp:139] Memory required for data: 5039360\n",
      "I20220425 13:10:08.990307 240875008 layer_factory.hpp:77] Creating layer relu1\n",
      "I20220425 13:10:08.990317 240875008 net.cpp:86] Creating Layer relu1\n",
      "I20220425 13:10:08.990322 240875008 net.cpp:408] relu1 <- ip1\n",
      "I20220425 13:10:08.990329 240875008 net.cpp:369] relu1 -> ip1 (in-place)\n",
      "I20220425 13:10:08.990335 240875008 net.cpp:124] Setting up relu1\n",
      "I20220425 13:10:08.990340 240875008 net.cpp:131] Top shape: 64 500 (32000)\n",
      "I20220425 13:10:08.990345 240875008 net.cpp:139] Memory required for data: 5167360\n",
      "I20220425 13:10:08.990350 240875008 layer_factory.hpp:77] Creating layer ip2\n",
      "I20220425 13:10:08.990356 240875008 net.cpp:86] Creating Layer ip2\n",
      "I20220425 13:10:08.990360 240875008 net.cpp:408] ip2 <- ip1\n",
      "I20220425 13:10:08.990365 240875008 net.cpp:382] ip2 -> ip2\n",
      "I20220425 13:10:08.990427 240875008 net.cpp:124] Setting up ip2\n",
      "I20220425 13:10:08.990432 240875008 net.cpp:131] Top shape: 64 10 (640)\n",
      "I20220425 13:10:08.990438 240875008 net.cpp:139] Memory required for data: 5169920\n",
      "I20220425 13:10:08.990443 240875008 layer_factory.hpp:77] Creating layer loss\n",
      "I20220425 13:10:08.990450 240875008 net.cpp:86] Creating Layer loss\n",
      "I20220425 13:10:08.990454 240875008 net.cpp:408] loss <- ip2\n",
      "I20220425 13:10:08.990459 240875008 net.cpp:408] loss <- label\n",
      "I20220425 13:10:08.990465 240875008 net.cpp:382] loss -> loss\n",
      "I20220425 13:10:08.990478 240875008 layer_factory.hpp:77] Creating layer loss\n",
      "I20220425 13:10:08.990495 240875008 net.cpp:124] Setting up loss\n",
      "I20220425 13:10:08.990540 240875008 net.cpp:131] Top shape: (1)\n",
      "I20220425 13:10:08.990553 240875008 net.cpp:134]     with loss weight 1\n",
      "I20220425 13:10:08.990561 240875008 net.cpp:139] Memory required for data: 5169924\n",
      "I20220425 13:10:08.990566 240875008 net.cpp:200] loss needs backward computation.\n",
      "I20220425 13:10:08.990571 240875008 net.cpp:200] ip2 needs backward computation.\n",
      "I20220425 13:10:08.990576 240875008 net.cpp:200] relu1 needs backward computation.\n",
      "I20220425 13:10:08.990581 240875008 net.cpp:200] ip1 needs backward computation.\n",
      "I20220425 13:10:08.990584 240875008 net.cpp:200] pool2 needs backward computation.\n",
      "I20220425 13:10:08.990589 240875008 net.cpp:200] conv2 needs backward computation.\n",
      "I20220425 13:10:08.990593 240875008 net.cpp:200] pool1 needs backward computation.\n",
      "I20220425 13:10:08.990598 240875008 net.cpp:200] conv1 needs backward computation.\n",
      "I20220425 13:10:08.990603 240875008 net.cpp:202] mnist does not need backward computation.\n",
      "I20220425 13:10:08.990607 240875008 net.cpp:244] This network produces output loss\n",
      "I20220425 13:10:08.990700 240875008 net.cpp:257] Network initialization done.\n",
      "I20220425 13:10:08.990731 240875008 caffe.cpp:351] Performing Forward\n",
      "I20220425 13:10:09.009819 240875008 caffe.cpp:356] Initial loss: 2.38076\n",
      "I20220425 13:10:09.009851 240875008 caffe.cpp:357] Performing Backward\n",
      "I20220425 13:10:09.029157 240875008 caffe.cpp:365] *** Benchmark begins ***\n",
      "I20220425 13:10:09.029198 240875008 caffe.cpp:366] Testing for 20 iterations.\n",
      "I20220425 13:10:09.059057 240875008 caffe.cpp:394] Iteration: 1 forward-backward time: 29 ms.\n",
      "I20220425 13:10:09.089699 240875008 caffe.cpp:394] Iteration: 2 forward-backward time: 30 ms.\n",
      "I20220425 13:10:09.117749 240875008 caffe.cpp:394] Iteration: 3 forward-backward time: 28 ms.\n",
      "I20220425 13:10:09.144970 240875008 caffe.cpp:394] Iteration: 4 forward-backward time: 27 ms.\n",
      "I20220425 13:10:09.170933 240875008 caffe.cpp:394] Iteration: 5 forward-backward time: 25 ms.\n",
      "I20220425 13:10:09.196146 240875008 caffe.cpp:394] Iteration: 6 forward-backward time: 25 ms.\n",
      "I20220425 13:10:09.222090 240875008 caffe.cpp:394] Iteration: 7 forward-backward time: 25 ms.\n",
      "I20220425 13:10:09.248469 240875008 caffe.cpp:394] Iteration: 8 forward-backward time: 26 ms.\n",
      "I20220425 13:10:09.273977 240875008 caffe.cpp:394] Iteration: 9 forward-backward time: 25 ms.\n",
      "I20220425 13:10:09.299454 240875008 caffe.cpp:394] Iteration: 10 forward-backward time: 25 ms.\n",
      "I20220425 13:10:09.325325 240875008 caffe.cpp:394] Iteration: 11 forward-backward time: 25 ms.\n",
      "I20220425 13:10:09.352089 240875008 caffe.cpp:394] Iteration: 12 forward-backward time: 26 ms.\n",
      "I20220425 13:10:09.378051 240875008 caffe.cpp:394] Iteration: 13 forward-backward time: 25 ms.\n",
      "I20220425 13:10:09.405395 240875008 caffe.cpp:394] Iteration: 14 forward-backward time: 27 ms.\n",
      "I20220425 13:10:09.431160 240875008 caffe.cpp:394] Iteration: 15 forward-backward time: 25 ms.\n",
      "I20220425 13:10:09.456243 240875008 caffe.cpp:394] Iteration: 16 forward-backward time: 25 ms.\n",
      "I20220425 13:10:09.482211 240875008 caffe.cpp:394] Iteration: 17 forward-backward time: 25 ms.\n",
      "I20220425 13:10:09.507026 240875008 caffe.cpp:394] Iteration: 18 forward-backward time: 24 ms.\n",
      "I20220425 13:10:09.532405 240875008 caffe.cpp:394] Iteration: 19 forward-backward time: 25 ms.\n",
      "I20220425 13:10:09.560453 240875008 caffe.cpp:394] Iteration: 20 forward-backward time: 28 ms.\n",
      "I20220425 13:10:09.560482 240875008 caffe.cpp:397] Average time per layer: \n",
      "I20220425 13:10:09.560487 240875008 caffe.cpp:400]      mnist\tforward: 0.00735 ms.\n",
      "I20220425 13:10:09.560492 240875008 caffe.cpp:403]      mnist\tbackward: 0.0006 ms.\n",
      "I20220425 13:10:09.560497 240875008 caffe.cpp:400]      conv1\tforward: 2.65035 ms.\n",
      "I20220425 13:10:09.560501 240875008 caffe.cpp:403]      conv1\tbackward: 1.6213 ms.\n",
      "I20220425 13:10:09.560505 240875008 caffe.cpp:400]      pool1\tforward: 3.4448 ms.\n",
      "I20220425 13:10:09.560510 240875008 caffe.cpp:403]      pool1\tbackward: 0.68645 ms.\n",
      "I20220425 13:10:09.560515 240875008 caffe.cpp:400]      conv2\tforward: 5.1095 ms.\n",
      "I20220425 13:10:09.560519 240875008 caffe.cpp:403]      conv2\tbackward: 9.0084 ms.\n",
      "I20220425 13:10:09.560524 240875008 caffe.cpp:400]      pool2\tforward: 1.92415 ms.\n",
      "I20220425 13:10:09.560528 240875008 caffe.cpp:403]      pool2\tbackward: 0.8736 ms.\n",
      "I20220425 13:10:09.560532 240875008 caffe.cpp:400]        ip1\tforward: 0.3818 ms.\n",
      "I20220425 13:10:09.560536 240875008 caffe.cpp:403]        ip1\tbackward: 0.5917 ms.\n",
      "I20220425 13:10:09.560541 240875008 caffe.cpp:400]      relu1\tforward: 0.02145 ms.\n",
      "I20220425 13:10:09.560545 240875008 caffe.cpp:403]      relu1\tbackward: 0.0386 ms.\n",
      "I20220425 13:10:09.560549 240875008 caffe.cpp:400]        ip2\tforward: 0.03635 ms.\n",
      "I20220425 13:10:09.560554 240875008 caffe.cpp:403]        ip2\tbackward: 0.0833 ms.\n",
      "I20220425 13:10:09.560557 240875008 caffe.cpp:400]       loss\tforward: 0.02295 ms.\n",
      "I20220425 13:10:09.560561 240875008 caffe.cpp:403]       loss\tbackward: 0.0013 ms.\n",
      "I20220425 13:10:09.560711 240875008 caffe.cpp:408] Average Forward pass: 13.6121 ms.\n",
      "I20220425 13:10:09.560719 240875008 caffe.cpp:410] Average Backward pass: 12.9194 ms.\n",
      "I20220425 13:10:09.560724 240875008 caffe.cpp:412] Average Forward-Backward: 26.55 ms.\n",
      "I20220425 13:10:09.560729 240875008 caffe.cpp:414] Total Time: 531 ms.\n",
      "I20220425 13:10:09.560806 240875008 caffe.cpp:415] *** Benchmark ends ***\n"
     ]
    }
   ],
   "source": [
    "!cd caffe;bash time_lenet.sh -iterations 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f550d-a2f8-442f-a33f-9ae7c59dc7ff",
   "metadata": {},
   "source": [
    "## 使用docker跑python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae40fa1-3873-41c4-9890-f5ae17addbcd",
   "metadata": {},
   "source": [
    "#### 安装\n",
    "\n",
    "```bash\n",
    "docker pull elezar/caffe:cpu\n",
    "```\n",
    "\n",
    "[readme](https://github.com/elezar/caffe/tree/master/docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd04cf-326a-4f5e-9a4d-ccb7f19cbbea",
   "metadata": {},
   "source": [
    "#### 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e68d61-0417-4bd2-ba53-36dda1a7a6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "libdc1394 error: Failed to initialize libdc1394\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I0425 11:38:34.566151     1 solver.cpp:48] Initializing solver from parameters: \n",
      "train_net: \"mnist/lenet_auto_train.prototxt\"\n",
      "test_net: \"mnist/lenet_auto_test.prototxt\"\n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.01\n",
      "display: 100\n",
      "max_iter: 10000\n",
      "lr_policy: \"inv\"\n",
      "gamma: 0.0001\n",
      "power: 0.75\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "snapshot: 5000\n",
      "snapshot_prefix: \"mnist/lenet\"\n",
      "I0425 11:38:34.569658     1 solver.cpp:81] Creating training net from train_net file: mnist/lenet_auto_train.prototxt\n",
      "I0425 11:38:34.574642     1 net.cpp:49] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TRAIN\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  transform_param {\n",
      "    scale: 0.0039215689\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"mnist/mnist_train_lmdb\"\n",
      "    batch_size: 64\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"fc1\"\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc1\"\n",
      "  top: \"fc1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"score\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc1\"\n",
      "  top: \"score\"\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"score\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0425 11:38:34.576678     1 layer_factory.hpp:77] Creating layer data\n",
      "I0425 11:38:34.578095     1 net.cpp:91] Creating Layer data\n",
      "I0425 11:38:34.578282     1 net.cpp:399] data -> data\n",
      "I0425 11:38:34.578373     1 net.cpp:399] data -> label\n",
      "I0425 11:38:34.587823     8 db_lmdb.cpp:35] Opened lmdb mnist/mnist_train_lmdb\n",
      "I0425 11:38:34.589061     1 data_layer.cpp:41] output data size: 64,1,28,28\n",
      "I0425 11:38:34.589767     1 net.cpp:141] Setting up data\n",
      "I0425 11:38:34.589810     1 net.cpp:148] Top shape: 64 1 28 28 (50176)\n",
      "I0425 11:38:34.589835     1 net.cpp:148] Top shape: 64 (64)\n",
      "I0425 11:38:34.589869     1 net.cpp:156] Memory required for data: 200960\n",
      "I0425 11:38:34.589928     1 layer_factory.hpp:77] Creating layer conv1\n",
      "I0425 11:38:34.590013     1 net.cpp:91] Creating Layer conv1\n",
      "I0425 11:38:34.590071     1 net.cpp:425] conv1 <- data\n",
      "I0425 11:38:34.590096     1 net.cpp:399] conv1 -> conv1\n",
      "I0425 11:38:34.590219     1 net.cpp:141] Setting up conv1\n",
      "I0425 11:38:34.590274     1 net.cpp:148] Top shape: 64 20 24 24 (737280)\n",
      "I0425 11:38:34.590288     1 net.cpp:156] Memory required for data: 3150080\n",
      "I0425 11:38:34.590327     1 layer_factory.hpp:77] Creating layer pool1\n",
      "I0425 11:38:34.590384     1 net.cpp:91] Creating Layer pool1\n",
      "I0425 11:38:34.590410     1 net.cpp:425] pool1 <- conv1\n",
      "I0425 11:38:34.590436     1 net.cpp:399] pool1 -> pool1\n",
      "I0425 11:38:34.590528     1 net.cpp:141] Setting up pool1\n",
      "I0425 11:38:34.590585     1 net.cpp:148] Top shape: 64 20 12 12 (184320)\n",
      "I0425 11:38:34.590698     1 net.cpp:156] Memory required for data: 3887360\n",
      "I0425 11:38:34.590754     1 layer_factory.hpp:77] Creating layer conv2\n",
      "I0425 11:38:34.590778     1 net.cpp:91] Creating Layer conv2\n",
      "I0425 11:38:34.590826     1 net.cpp:425] conv2 <- pool1\n",
      "I0425 11:38:34.590847     1 net.cpp:399] conv2 -> conv2\n",
      "I0425 11:38:34.592175     1 net.cpp:141] Setting up conv2\n",
      "I0425 11:38:34.592243     1 net.cpp:148] Top shape: 64 50 8 8 (204800)\n",
      "I0425 11:38:34.592270     1 net.cpp:156] Memory required for data: 4706560\n",
      "I0425 11:38:34.592294     1 layer_factory.hpp:77] Creating layer pool2\n",
      "I0425 11:38:34.592342     1 net.cpp:91] Creating Layer pool2\n",
      "I0425 11:38:34.592418     1 net.cpp:425] pool2 <- conv2\n",
      "I0425 11:38:34.592474     1 net.cpp:399] pool2 -> pool2\n",
      "I0425 11:38:34.592557     1 net.cpp:141] Setting up pool2\n",
      "I0425 11:38:34.592618     1 net.cpp:148] Top shape: 64 50 4 4 (51200)\n",
      "I0425 11:38:34.592640     1 net.cpp:156] Memory required for data: 4911360\n",
      "I0425 11:38:34.592671     1 layer_factory.hpp:77] Creating layer fc1\n",
      "I0425 11:38:34.592738     1 net.cpp:91] Creating Layer fc1\n",
      "I0425 11:38:34.592764     1 net.cpp:425] fc1 <- pool2\n",
      "I0425 11:38:34.592778     1 net.cpp:399] fc1 -> fc1\n",
      "I0425 11:38:34.596560     1 net.cpp:141] Setting up fc1\n",
      "I0425 11:38:34.596601     1 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0425 11:38:34.596619     1 net.cpp:156] Memory required for data: 5039360\n",
      "I0425 11:38:34.596712     1 layer_factory.hpp:77] Creating layer relu1\n",
      "I0425 11:38:34.596745     1 net.cpp:91] Creating Layer relu1\n",
      "I0425 11:38:34.596765     1 net.cpp:425] relu1 <- fc1\n",
      "I0425 11:38:34.596805     1 net.cpp:386] relu1 -> fc1 (in-place)\n",
      "I0425 11:38:34.596879     1 net.cpp:141] Setting up relu1\n",
      "I0425 11:38:34.596908     1 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0425 11:38:34.596935     1 net.cpp:156] Memory required for data: 5167360\n",
      "I0425 11:38:34.596954     1 layer_factory.hpp:77] Creating layer score\n",
      "I0425 11:38:34.596966     1 net.cpp:91] Creating Layer score\n",
      "I0425 11:38:34.596976     1 net.cpp:425] score <- fc1\n",
      "I0425 11:38:34.596984     1 net.cpp:399] score -> score\n",
      "I0425 11:38:34.597054     1 net.cpp:141] Setting up score\n",
      "I0425 11:38:34.597069     1 net.cpp:148] Top shape: 64 10 (640)\n",
      "I0425 11:38:34.597079     1 net.cpp:156] Memory required for data: 5169920\n",
      "I0425 11:38:34.597100     1 layer_factory.hpp:77] Creating layer loss\n",
      "I0425 11:38:34.597126     1 net.cpp:91] Creating Layer loss\n",
      "I0425 11:38:34.597146     1 net.cpp:425] loss <- score\n",
      "I0425 11:38:34.597167     1 net.cpp:425] loss <- label\n",
      "I0425 11:38:34.597185     1 net.cpp:399] loss -> loss\n",
      "I0425 11:38:34.597227     1 layer_factory.hpp:77] Creating layer loss\n",
      "I0425 11:38:34.597283     1 net.cpp:141] Setting up loss\n",
      "I0425 11:38:34.597513     1 net.cpp:148] Top shape: (1)\n",
      "I0425 11:38:34.597541     1 net.cpp:151]     with loss weight 1\n",
      "I0425 11:38:34.597563     1 net.cpp:156] Memory required for data: 5169924\n",
      "I0425 11:38:34.597581     1 net.cpp:217] loss needs backward computation.\n",
      "I0425 11:38:34.597606     1 net.cpp:217] score needs backward computation.\n",
      "I0425 11:38:34.597617     1 net.cpp:217] relu1 needs backward computation.\n",
      "I0425 11:38:34.597627     1 net.cpp:217] fc1 needs backward computation.\n",
      "I0425 11:38:34.597640     1 net.cpp:217] pool2 needs backward computation.\n",
      "I0425 11:38:34.597656     1 net.cpp:217] conv2 needs backward computation.\n",
      "I0425 11:38:34.597720     1 net.cpp:217] pool1 needs backward computation.\n",
      "I0425 11:38:34.597750     1 net.cpp:217] conv1 needs backward computation.\n",
      "I0425 11:38:34.597784     1 net.cpp:219] data does not need backward computation.\n",
      "I0425 11:38:34.597826     1 net.cpp:261] This network produces output loss\n",
      "I0425 11:38:34.597858     1 net.cpp:274] Network initialization done.\n",
      "I0425 11:38:34.603163     1 solver.cpp:181] Creating test net (#0) specified by test_net file: mnist/lenet_auto_test.prototxt\n",
      "I0425 11:38:34.603385     1 net.cpp:49] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  transform_param {\n",
      "    scale: 0.0039215689\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"mnist/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"fc1\"\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc1\"\n",
      "  top: \"fc1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"score\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc1\"\n",
      "  top: \"score\"\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"score\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0425 11:38:34.603960     1 layer_factory.hpp:77] Creating layer data\n",
      "I0425 11:38:34.604684     1 net.cpp:91] Creating Layer data\n",
      "I0425 11:38:34.604709     1 net.cpp:399] data -> data\n",
      "I0425 11:38:34.604729     1 net.cpp:399] data -> label\n",
      "I0425 11:38:34.621713    10 db_lmdb.cpp:35] Opened lmdb mnist/mnist_test_lmdb\n",
      "I0425 11:38:34.623308     1 data_layer.cpp:41] output data size: 100,1,28,28\n",
      "I0425 11:38:34.624312     1 net.cpp:141] Setting up data\n",
      "I0425 11:38:34.624408     1 net.cpp:148] Top shape: 100 1 28 28 (78400)\n",
      "I0425 11:38:34.624436     1 net.cpp:148] Top shape: 100 (100)\n",
      "I0425 11:38:34.624449     1 net.cpp:156] Memory required for data: 314000\n",
      "I0425 11:38:34.624485     1 layer_factory.hpp:77] Creating layer conv1\n",
      "I0425 11:38:34.624845     1 net.cpp:91] Creating Layer conv1\n",
      "I0425 11:38:34.624899     1 net.cpp:425] conv1 <- data\n",
      "I0425 11:38:34.624933     1 net.cpp:399] conv1 -> conv1\n",
      "I0425 11:38:34.625190     1 net.cpp:141] Setting up conv1\n",
      "I0425 11:38:34.625296     1 net.cpp:148] Top shape: 100 20 24 24 (1152000)\n",
      "I0425 11:38:34.625329     1 net.cpp:156] Memory required for data: 4922000\n",
      "I0425 11:38:34.625578     1 layer_factory.hpp:77] Creating layer pool1\n",
      "I0425 11:38:34.625624     1 net.cpp:91] Creating Layer pool1\n",
      "I0425 11:38:34.625653     1 net.cpp:425] pool1 <- conv1\n",
      "I0425 11:38:34.625679     1 net.cpp:399] pool1 -> pool1\n",
      "I0425 11:38:34.625788     1 net.cpp:141] Setting up pool1\n",
      "I0425 11:38:34.625815     1 net.cpp:148] Top shape: 100 20 12 12 (288000)\n",
      "I0425 11:38:34.625859     1 net.cpp:156] Memory required for data: 6074000\n",
      "I0425 11:38:34.626566     1 layer_factory.hpp:77] Creating layer conv2\n",
      "I0425 11:38:34.626621     1 net.cpp:91] Creating Layer conv2\n",
      "I0425 11:38:34.626750     1 net.cpp:425] conv2 <- pool1\n",
      "I0425 11:38:34.626899     1 net.cpp:399] conv2 -> conv2\n",
      "I0425 11:38:34.627272     1 net.cpp:141] Setting up conv2\n",
      "I0425 11:38:34.627372     1 net.cpp:148] Top shape: 100 50 8 8 (320000)\n",
      "I0425 11:38:34.627404     1 net.cpp:156] Memory required for data: 7354000\n",
      "I0425 11:38:34.627439     1 layer_factory.hpp:77] Creating layer pool2\n",
      "I0425 11:38:34.627470     1 net.cpp:91] Creating Layer pool2\n",
      "I0425 11:38:34.627496     1 net.cpp:425] pool2 <- conv2\n",
      "I0425 11:38:34.628039     1 net.cpp:399] pool2 -> pool2\n",
      "I0425 11:38:34.628091     1 net.cpp:141] Setting up pool2\n",
      "I0425 11:38:34.628139     1 net.cpp:148] Top shape: 100 50 4 4 (80000)\n",
      "I0425 11:38:34.628214     1 net.cpp:156] Memory required for data: 7674000\n",
      "I0425 11:38:34.628249     1 layer_factory.hpp:77] Creating layer fc1\n",
      "I0425 11:38:34.628271     1 net.cpp:91] Creating Layer fc1\n",
      "I0425 11:38:34.628289     1 net.cpp:425] fc1 <- pool2\n",
      "I0425 11:38:34.628314     1 net.cpp:399] fc1 -> fc1\n",
      "I0425 11:38:34.631950     1 net.cpp:141] Setting up fc1\n",
      "I0425 11:38:34.632050     1 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0425 11:38:34.632068     1 net.cpp:156] Memory required for data: 7874000\n",
      "I0425 11:38:34.632095     1 layer_factory.hpp:77] Creating layer relu1\n",
      "I0425 11:38:34.632115     1 net.cpp:91] Creating Layer relu1\n",
      "I0425 11:38:34.632133     1 net.cpp:425] relu1 <- fc1\n",
      "I0425 11:38:34.632216     1 net.cpp:386] relu1 -> fc1 (in-place)\n",
      "I0425 11:38:34.632254     1 net.cpp:141] Setting up relu1\n",
      "I0425 11:38:34.632277     1 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0425 11:38:34.632298     1 net.cpp:156] Memory required for data: 8074000\n",
      "I0425 11:38:34.632329     1 layer_factory.hpp:77] Creating layer score\n",
      "I0425 11:38:34.632428     1 net.cpp:91] Creating Layer score\n",
      "I0425 11:38:34.632494     1 net.cpp:425] score <- fc1\n",
      "I0425 11:38:34.632529     1 net.cpp:399] score -> score\n",
      "I0425 11:38:34.632630     1 net.cpp:141] Setting up score\n",
      "I0425 11:38:34.632767     1 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0425 11:38:34.632804     1 net.cpp:156] Memory required for data: 8078000\n",
      "I0425 11:38:34.632936     1 layer_factory.hpp:77] Creating layer loss\n",
      "I0425 11:38:34.633009     1 net.cpp:91] Creating Layer loss\n",
      "I0425 11:38:34.633033     1 net.cpp:425] loss <- score\n",
      "I0425 11:38:34.633105     1 net.cpp:425] loss <- label\n",
      "I0425 11:38:34.633175     1 net.cpp:399] loss -> loss\n",
      "I0425 11:38:34.633272     1 layer_factory.hpp:77] Creating layer loss\n",
      "I0425 11:38:34.633361     1 net.cpp:141] Setting up loss\n",
      "I0425 11:38:34.633459     1 net.cpp:148] Top shape: (1)\n",
      "I0425 11:38:34.633522     1 net.cpp:151]     with loss weight 1\n",
      "I0425 11:38:34.633589     1 net.cpp:156] Memory required for data: 8078004\n",
      "I0425 11:38:34.633627     1 net.cpp:217] loss needs backward computation.\n",
      "I0425 11:38:34.633692     1 net.cpp:217] score needs backward computation.\n",
      "I0425 11:38:34.633769     1 net.cpp:217] relu1 needs backward computation.\n",
      "I0425 11:38:34.633843     1 net.cpp:217] fc1 needs backward computation.\n",
      "I0425 11:38:34.633940     1 net.cpp:217] pool2 needs backward computation.\n",
      "I0425 11:38:34.634011     1 net.cpp:217] conv2 needs backward computation.\n",
      "I0425 11:38:34.634079     1 net.cpp:217] pool1 needs backward computation.\n",
      "I0425 11:38:34.634109     1 net.cpp:217] conv1 needs backward computation.\n",
      "I0425 11:38:34.634171     1 net.cpp:219] data does not need backward computation.\n",
      "I0425 11:38:34.634290     1 net.cpp:261] This network produces output loss\n",
      "I0425 11:38:34.634330     1 net.cpp:274] Network initialization done.\n",
      "I0425 11:38:34.634397     1 solver.cpp:60] Solver scaffolding done.\n",
      "=== net blobs ===\n",
      "data (64, 1, 28, 28)\n",
      "label (64,)\n",
      "conv1 (64, 20, 24, 24)\n",
      "pool1 (64, 20, 12, 12)\n",
      "conv2 (64, 50, 8, 8)\n",
      "pool2 (64, 50, 4, 4)\n",
      "fc1 (64, 500)\n",
      "score (64, 10)\n",
      "loss ()\n",
      "=== net params ===\n",
      "conv1 (20, 1, 5, 5)\n",
      "conv2 (50, 20, 5, 5)\n",
      "fc1 (500, 800)\n",
      "score (10, 500)\n",
      "=== forward result ===\n",
      "{'loss': array(2.3930859565734863, dtype=float32)}\n",
      "=== train labels ===\n",
      "[ 5.  0.  4.  1.  9.  2.  1.  3.]\n",
      "=== test labels ===\n",
      "[ 7.  2.  1.  0.  4.  1.  4.  9.]\n",
      "=== training and testing ===\n",
      "I0425 11:38:34.757921     1 solver.cpp:337] Iteration 0, Testing net (#0)\n",
      "I0425 11:38:39.379254     1 solver.cpp:404]     Test net output #0: loss = 2.41447 (* 1 = 2.41447 loss)\n",
      "I0425 11:38:39.448352     1 solver.cpp:228] Iteration 0, loss = 2.43979\n",
      "I0425 11:38:39.448498     1 solver.cpp:244]     Train net output #0: loss = 2.43979 (* 1 = 2.43979 loss)\n",
      "I0425 11:38:39.448527     1 sgd_solver.cpp:106] Iteration 0, lr = 0.01\n",
      "Iteration 0 testing...\n",
      "Iteration 25 testing...\n",
      "Iteration 50 testing...\n",
      "Iteration 75 testing...\n",
      "I0425 11:39:07.765923     1 solver.cpp:228] Iteration 100, loss = 0.260404\n",
      "I0425 11:39:07.765996     1 solver.cpp:244]     Train net output #0: loss = 0.260404 (* 1 = 0.260404 loss)\n",
      "I0425 11:39:07.766018     1 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565\n",
      "Iteration 100 testing...\n",
      "Iteration 125 testing...\n",
      "Iteration 150 testing...\n",
      "Iteration 175 testing...\n",
      "=== train loss ===\n",
      "[ 2.43978834  2.3888166   2.24742723  2.21557927  2.10527992  2.05270648\n",
      "  2.01566672  1.95901513  1.98840904  1.71722078  1.64003694  1.53109121\n",
      "  1.53364205  1.41227567  1.27494037  1.3035785   1.32998443  0.93663597\n",
      "  1.10609806  0.99307519  0.97086829  0.77847314  0.79184705  0.74589598\n",
      "  0.69452488  0.50801593  0.51332867  0.58897471  0.56730217  0.55030429\n",
      "  0.61457437  0.53258419  0.36818177  0.44711706  0.41343394  0.38906711\n",
      "  0.69590068  0.53346699  0.32564741  0.33754683  0.73188752  0.5346269\n",
      "  0.50807184  0.25551805  0.50344658  0.31245768  0.51408064  0.31496477\n",
      "  0.22756045  0.30779263  0.38382164  0.45641682  0.3168501   0.35573846\n",
      "  0.40699431  0.25729704  0.52488023  0.43835837  0.43387571  0.23113398\n",
      "  0.22966252  0.42799655  0.43694577  0.37693757  0.3384012   0.33705604\n",
      "  0.28208643  0.42886069  0.30145079  0.24624105  0.19660811  0.37586057\n",
      "  0.29334572  0.27180585  0.27288118  0.18941838  0.52757078  0.30141592\n",
      "  0.41043168  0.6679492   0.15683815  0.45069948  0.2740964   0.20379204\n",
      "  0.24433815  0.36530259  0.28612491  0.20179524  0.22635227  0.16150655\n",
      "  0.36715397  0.23783693  0.10105921  0.16503043  0.20559581  0.09965935\n",
      "  0.23669556  0.2274005   0.16722022  0.21272464  0.26040432  0.13599089\n",
      "  0.15834539  0.2598702   0.11433892  0.28594613  0.43655801  0.42778352\n",
      "  0.46716088  0.20049594  0.28228158  0.31178582  0.36295599  0.26710299\n",
      "  0.20617121  0.16232608  0.24794227  0.18501717  0.18647082  0.19888577\n",
      "  0.18311466  0.32758752  0.39925429  0.23612551  0.19669265  0.36662236\n",
      "  0.142456    0.44264027  0.22808832  0.11491006  0.22210412  0.39414009\n",
      "  0.16734995  0.13669522  0.34480122  0.56568009  0.35743794  0.52180654\n",
      "  0.31262273  0.19421805  0.1060093   0.43676898  0.147238    0.29501832\n",
      "  0.31360212  0.32444316  0.43916637  0.20382869  0.31205004  0.28887337\n",
      "  0.07742298  0.34160286  0.06179618  0.16770986  0.20596649  0.26201612\n",
      "  0.15877724  0.23641326  0.44932672  0.2912468   0.1319378   0.11051235\n",
      "  0.40270326  0.16104488  0.30219454  0.28638795  0.55615085  0.3015666\n",
      "  0.28114888  0.23921253  0.27550498  0.18311717  0.2575022   0.17503957\n",
      "  0.33975428  0.14830388  0.14719501  0.08147377  0.14769466  0.40345344\n",
      "  0.2495258   0.36936533  0.29043967  0.0933549   0.24944659  0.11372892\n",
      "  0.13263841  0.23306896  0.08756168  0.22716427  0.18170388  0.1401599\n",
      "  0.21643858  0.17867075  0.21395329  0.41388667  0.25902906  0.25109652\n",
      "  0.18671483  0.14478175]\n",
      "=== test acc ===\n",
      "[ 0.128   0.7823  0.8699  0.9043  0.9179  0.9343  0.9306  0.9401]\n"
     ]
    }
   ],
   "source": [
    "!cd caffe;bash run_docker_python.sh mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28cd2d-b404-44f7-b187-5200c7761737",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbcb60ac-f39b-451b-bdad-8ef76dcd043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "libdc1394 error: Failed to initialize libdc1394\n",
      "CaffeNet Model Found.\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I0425 12:43:34.115283     1 net.cpp:49] Initializing net from parameters: \n",
      "name: \"CaffeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Input\"\n",
      "  top: \"data\"\n",
      "  input_param {\n",
      "    shape {\n",
      "      dim: 10\n",
      "      dim: 3\n",
      "      dim: 227\n",
      "      dim: 227\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  convolution_param {\n",
      "    num_output: 96\n",
      "    kernel_size: 11\n",
      "    stride: 4\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"conv1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"conv2\"\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    group: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv4\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv4\"\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu4\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv4\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv5\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv5\"\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu5\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"conv5\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool5\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"pool5\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc6\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool5\"\n",
      "  top: \"fc6\"\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu6\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop6\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc7\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc7\"\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu7\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop7\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc8\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc8\"\n",
      "  inner_product_param {\n",
      "    num_output: 1000\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"prob\"\n",
      "  type: \"Softmax\"\n",
      "  bottom: \"fc8\"\n",
      "  top: \"prob\"\n",
      "}\n",
      "I0425 12:43:34.115494     1 layer_factory.hpp:77] Creating layer data\n",
      "I0425 12:43:34.115548     1 net.cpp:91] Creating Layer data\n",
      "I0425 12:43:34.115581     1 net.cpp:399] data -> data\n",
      "I0425 12:43:34.115804     1 net.cpp:141] Setting up data\n",
      "I0425 12:43:34.115929     1 net.cpp:148] Top shape: 10 3 227 227 (1545870)\n",
      "I0425 12:43:34.115947     1 net.cpp:156] Memory required for data: 6183480\n",
      "I0425 12:43:34.115963     1 layer_factory.hpp:77] Creating layer conv1\n",
      "I0425 12:43:34.116067     1 net.cpp:91] Creating Layer conv1\n",
      "I0425 12:43:34.116117     1 net.cpp:425] conv1 <- data\n",
      "I0425 12:43:34.116137     1 net.cpp:399] conv1 -> conv1\n",
      "I0425 12:43:34.116269     1 net.cpp:141] Setting up conv1\n",
      "I0425 12:43:34.116319     1 net.cpp:148] Top shape: 10 96 55 55 (2904000)\n",
      "I0425 12:43:34.116329     1 net.cpp:156] Memory required for data: 17799480\n",
      "I0425 12:43:34.116349     1 layer_factory.hpp:77] Creating layer relu1\n",
      "I0425 12:43:34.116365     1 net.cpp:91] Creating Layer relu1\n",
      "I0425 12:43:34.116374     1 net.cpp:425] relu1 <- conv1\n",
      "I0425 12:43:34.116382     1 net.cpp:386] relu1 -> conv1 (in-place)\n",
      "I0425 12:43:34.116401     1 net.cpp:141] Setting up relu1\n",
      "I0425 12:43:34.116412     1 net.cpp:148] Top shape: 10 96 55 55 (2904000)\n",
      "I0425 12:43:34.116420     1 net.cpp:156] Memory required for data: 29415480\n",
      "I0425 12:43:34.116459     1 layer_factory.hpp:77] Creating layer pool1\n",
      "I0425 12:43:34.116477     1 net.cpp:91] Creating Layer pool1\n",
      "I0425 12:43:34.116497     1 net.cpp:425] pool1 <- conv1\n",
      "I0425 12:43:34.116513     1 net.cpp:399] pool1 -> pool1\n",
      "I0425 12:43:34.116530     1 net.cpp:141] Setting up pool1\n",
      "I0425 12:43:34.116539     1 net.cpp:148] Top shape: 10 96 27 27 (699840)\n",
      "I0425 12:43:34.116547     1 net.cpp:156] Memory required for data: 32214840\n",
      "I0425 12:43:34.116555     1 layer_factory.hpp:77] Creating layer norm1\n",
      "I0425 12:43:34.116570     1 net.cpp:91] Creating Layer norm1\n",
      "I0425 12:43:34.116590     1 net.cpp:425] norm1 <- pool1\n",
      "I0425 12:43:34.116611     1 net.cpp:399] norm1 -> norm1\n",
      "I0425 12:43:34.116636     1 net.cpp:141] Setting up norm1\n",
      "I0425 12:43:34.116652     1 net.cpp:148] Top shape: 10 96 27 27 (699840)\n",
      "I0425 12:43:34.116662     1 net.cpp:156] Memory required for data: 35014200\n",
      "I0425 12:43:34.116669     1 layer_factory.hpp:77] Creating layer conv2\n",
      "I0425 12:43:34.116714     1 net.cpp:91] Creating Layer conv2\n",
      "I0425 12:43:34.116725     1 net.cpp:425] conv2 <- norm1\n",
      "I0425 12:43:34.116732     1 net.cpp:399] conv2 -> conv2\n",
      "I0425 12:43:34.117309     1 net.cpp:141] Setting up conv2\n",
      "I0425 12:43:34.117359     1 net.cpp:148] Top shape: 10 256 27 27 (1866240)\n",
      "I0425 12:43:34.117368     1 net.cpp:156] Memory required for data: 42479160\n",
      "I0425 12:43:34.117381     1 layer_factory.hpp:77] Creating layer relu2\n",
      "I0425 12:43:34.117389     1 net.cpp:91] Creating Layer relu2\n",
      "I0425 12:43:34.117399     1 net.cpp:425] relu2 <- conv2\n",
      "I0425 12:43:34.117408     1 net.cpp:386] relu2 -> conv2 (in-place)\n",
      "I0425 12:43:34.117415     1 net.cpp:141] Setting up relu2\n",
      "I0425 12:43:34.117455     1 net.cpp:148] Top shape: 10 256 27 27 (1866240)\n",
      "I0425 12:43:34.117463     1 net.cpp:156] Memory required for data: 49944120\n",
      "I0425 12:43:34.117473     1 layer_factory.hpp:77] Creating layer pool2\n",
      "I0425 12:43:34.117481     1 net.cpp:91] Creating Layer pool2\n",
      "I0425 12:43:34.117494     1 net.cpp:425] pool2 <- conv2\n",
      "I0425 12:43:34.117517     1 net.cpp:399] pool2 -> pool2\n",
      "I0425 12:43:34.117542     1 net.cpp:141] Setting up pool2\n",
      "I0425 12:43:34.117563     1 net.cpp:148] Top shape: 10 256 13 13 (432640)\n",
      "I0425 12:43:34.117581     1 net.cpp:156] Memory required for data: 51674680\n",
      "I0425 12:43:34.117597     1 layer_factory.hpp:77] Creating layer norm2\n",
      "I0425 12:43:34.117614     1 net.cpp:91] Creating Layer norm2\n",
      "I0425 12:43:34.117625     1 net.cpp:425] norm2 <- pool2\n",
      "I0425 12:43:34.117633     1 net.cpp:399] norm2 -> norm2\n",
      "I0425 12:43:34.117677     1 net.cpp:141] Setting up norm2\n",
      "I0425 12:43:34.117695     1 net.cpp:148] Top shape: 10 256 13 13 (432640)\n",
      "I0425 12:43:34.117712     1 net.cpp:156] Memory required for data: 53405240\n",
      "I0425 12:43:34.117720     1 layer_factory.hpp:77] Creating layer conv3\n",
      "I0425 12:43:34.117736     1 net.cpp:91] Creating Layer conv3\n",
      "I0425 12:43:34.117756     1 net.cpp:425] conv3 <- norm2\n",
      "I0425 12:43:34.117774     1 net.cpp:399] conv3 -> conv3\n",
      "I0425 12:43:34.119320     1 net.cpp:141] Setting up conv3\n",
      "I0425 12:43:34.119369     1 net.cpp:148] Top shape: 10 384 13 13 (648960)\n",
      "I0425 12:43:34.119377     1 net.cpp:156] Memory required for data: 56001080\n",
      "I0425 12:43:34.119395     1 layer_factory.hpp:77] Creating layer relu3\n",
      "I0425 12:43:34.119402     1 net.cpp:91] Creating Layer relu3\n",
      "I0425 12:43:34.119413     1 net.cpp:425] relu3 <- conv3\n",
      "I0425 12:43:34.119421     1 net.cpp:386] relu3 -> conv3 (in-place)\n",
      "I0425 12:43:34.119437     1 net.cpp:141] Setting up relu3\n",
      "I0425 12:43:34.119459     1 net.cpp:148] Top shape: 10 384 13 13 (648960)\n",
      "I0425 12:43:34.119477     1 net.cpp:156] Memory required for data: 58596920\n",
      "I0425 12:43:34.119493     1 layer_factory.hpp:77] Creating layer conv4\n",
      "I0425 12:43:34.119510     1 net.cpp:91] Creating Layer conv4\n",
      "I0425 12:43:34.119521     1 net.cpp:425] conv4 <- conv3\n",
      "I0425 12:43:34.119529     1 net.cpp:399] conv4 -> conv4\n",
      "I0425 12:43:34.120898     1 net.cpp:141] Setting up conv4\n",
      "I0425 12:43:34.120970     1 net.cpp:148] Top shape: 10 384 13 13 (648960)\n",
      "I0425 12:43:34.120993     1 net.cpp:156] Memory required for data: 61192760\n",
      "I0425 12:43:34.121019     1 layer_factory.hpp:77] Creating layer relu4\n",
      "I0425 12:43:34.121042     1 net.cpp:91] Creating Layer relu4\n",
      "I0425 12:43:34.121066     1 net.cpp:425] relu4 <- conv4\n",
      "I0425 12:43:34.121088     1 net.cpp:386] relu4 -> conv4 (in-place)\n",
      "I0425 12:43:34.121110     1 net.cpp:141] Setting up relu4\n",
      "I0425 12:43:34.121129     1 net.cpp:148] Top shape: 10 384 13 13 (648960)\n",
      "I0425 12:43:34.121140     1 net.cpp:156] Memory required for data: 63788600\n",
      "I0425 12:43:34.121147     1 layer_factory.hpp:77] Creating layer conv5\n",
      "I0425 12:43:34.121165     1 net.cpp:91] Creating Layer conv5\n",
      "I0425 12:43:34.121176     1 net.cpp:425] conv5 <- conv4\n",
      "I0425 12:43:34.121183     1 net.cpp:399] conv5 -> conv5\n",
      "I0425 12:43:34.122305     1 net.cpp:141] Setting up conv5\n",
      "I0425 12:43:34.122376     1 net.cpp:148] Top shape: 10 256 13 13 (432640)\n",
      "I0425 12:43:34.122400     1 net.cpp:156] Memory required for data: 65519160\n",
      "I0425 12:43:34.122450     1 layer_factory.hpp:77] Creating layer relu5\n",
      "I0425 12:43:34.122470     1 net.cpp:91] Creating Layer relu5\n",
      "I0425 12:43:34.122490     1 net.cpp:425] relu5 <- conv5\n",
      "I0425 12:43:34.122519     1 net.cpp:386] relu5 -> conv5 (in-place)\n",
      "I0425 12:43:34.122536     1 net.cpp:141] Setting up relu5\n",
      "I0425 12:43:34.122553     1 net.cpp:148] Top shape: 10 256 13 13 (432640)\n",
      "I0425 12:43:34.122573     1 net.cpp:156] Memory required for data: 67249720\n",
      "I0425 12:43:34.122583     1 layer_factory.hpp:77] Creating layer pool5\n",
      "I0425 12:43:34.122956     1 net.cpp:91] Creating Layer pool5\n",
      "I0425 12:43:34.123045     1 net.cpp:425] pool5 <- conv5\n",
      "I0425 12:43:34.123087     1 net.cpp:399] pool5 -> pool5\n",
      "I0425 12:43:34.123126     1 net.cpp:141] Setting up pool5\n",
      "I0425 12:43:34.123195     1 net.cpp:148] Top shape: 10 256 6 6 (92160)\n",
      "I0425 12:43:34.123225     1 net.cpp:156] Memory required for data: 67618360\n",
      "I0425 12:43:34.123447     1 layer_factory.hpp:77] Creating layer fc6\n",
      "I0425 12:43:34.123489     1 net.cpp:91] Creating Layer fc6\n",
      "I0425 12:43:34.123514     1 net.cpp:425] fc6 <- pool5\n",
      "I0425 12:43:34.123622     1 net.cpp:399] fc6 -> fc6\n",
      "I0425 12:43:34.194304     1 net.cpp:141] Setting up fc6\n",
      "I0425 12:43:34.194378     1 net.cpp:148] Top shape: 10 4096 (40960)\n",
      "I0425 12:43:34.194397     1 net.cpp:156] Memory required for data: 67782200\n",
      "I0425 12:43:34.194423     1 layer_factory.hpp:77] Creating layer relu6\n",
      "I0425 12:43:34.194450     1 net.cpp:91] Creating Layer relu6\n",
      "I0425 12:43:34.194496     1 net.cpp:425] relu6 <- fc6\n",
      "I0425 12:43:34.194516     1 net.cpp:386] relu6 -> fc6 (in-place)\n",
      "I0425 12:43:34.194541     1 net.cpp:141] Setting up relu6\n",
      "I0425 12:43:34.194550     1 net.cpp:148] Top shape: 10 4096 (40960)\n",
      "I0425 12:43:34.194561     1 net.cpp:156] Memory required for data: 67946040\n",
      "I0425 12:43:34.194568     1 layer_factory.hpp:77] Creating layer drop6\n",
      "I0425 12:43:34.194593     1 net.cpp:91] Creating Layer drop6\n",
      "I0425 12:43:34.194613     1 net.cpp:425] drop6 <- fc6\n",
      "I0425 12:43:34.194631     1 net.cpp:386] drop6 -> fc6 (in-place)\n",
      "I0425 12:43:34.194648     1 net.cpp:141] Setting up drop6\n",
      "I0425 12:43:34.194662     1 net.cpp:148] Top shape: 10 4096 (40960)\n",
      "I0425 12:43:34.194681     1 net.cpp:156] Memory required for data: 68109880\n",
      "I0425 12:43:34.194700     1 layer_factory.hpp:77] Creating layer fc7\n",
      "I0425 12:43:34.195307     1 net.cpp:91] Creating Layer fc7\n",
      "I0425 12:43:34.195317     1 net.cpp:425] fc7 <- fc6\n",
      "I0425 12:43:34.195325     1 net.cpp:399] fc7 -> fc7\n",
      "I0425 12:43:34.226393     1 net.cpp:141] Setting up fc7\n",
      "I0425 12:43:34.226477     1 net.cpp:148] Top shape: 10 4096 (40960)\n",
      "I0425 12:43:34.226511     1 net.cpp:156] Memory required for data: 68273720\n",
      "I0425 12:43:34.226536     1 layer_factory.hpp:77] Creating layer relu7\n",
      "I0425 12:43:34.226595     1 net.cpp:91] Creating Layer relu7\n",
      "I0425 12:43:34.226611     1 net.cpp:425] relu7 <- fc7\n",
      "I0425 12:43:34.226622     1 net.cpp:386] relu7 -> fc7 (in-place)\n",
      "I0425 12:43:34.226642     1 net.cpp:141] Setting up relu7\n",
      "I0425 12:43:34.226665     1 net.cpp:148] Top shape: 10 4096 (40960)\n",
      "I0425 12:43:34.226714     1 net.cpp:156] Memory required for data: 68437560\n",
      "I0425 12:43:34.226723     1 layer_factory.hpp:77] Creating layer drop7\n",
      "I0425 12:43:34.226758     1 net.cpp:91] Creating Layer drop7\n",
      "I0425 12:43:34.226768     1 net.cpp:425] drop7 <- fc7\n",
      "I0425 12:43:34.226783     1 net.cpp:386] drop7 -> fc7 (in-place)\n",
      "I0425 12:43:34.226992     1 net.cpp:141] Setting up drop7\n",
      "I0425 12:43:34.227092     1 net.cpp:148] Top shape: 10 4096 (40960)\n",
      "I0425 12:43:34.227113     1 net.cpp:156] Memory required for data: 68601400\n",
      "I0425 12:43:34.227980     1 layer_factory.hpp:77] Creating layer fc8\n",
      "I0425 12:43:34.228063     1 net.cpp:91] Creating Layer fc8\n",
      "I0425 12:43:34.228121     1 net.cpp:425] fc8 <- fc7\n",
      "I0425 12:43:34.228147     1 net.cpp:399] fc8 -> fc8\n",
      "I0425 12:43:34.234944     1 net.cpp:141] Setting up fc8\n",
      "I0425 12:43:34.235005     1 net.cpp:148] Top shape: 10 1000 (10000)\n",
      "I0425 12:43:34.235014     1 net.cpp:156] Memory required for data: 68641400\n",
      "I0425 12:43:34.235031     1 layer_factory.hpp:77] Creating layer prob\n",
      "I0425 12:43:34.235054     1 net.cpp:91] Creating Layer prob\n",
      "I0425 12:43:34.235096     1 net.cpp:425] prob <- fc8\n",
      "I0425 12:43:34.235116     1 net.cpp:399] prob -> prob\n",
      "I0425 12:43:34.235143     1 net.cpp:141] Setting up prob\n",
      "I0425 12:43:34.235193     1 net.cpp:148] Top shape: 10 1000 (10000)\n",
      "I0425 12:43:34.235205     1 net.cpp:156] Memory required for data: 68681400\n",
      "I0425 12:43:34.235219     1 net.cpp:219] prob does not need backward computation.\n",
      "I0425 12:43:34.235237     1 net.cpp:219] fc8 does not need backward computation.\n",
      "I0425 12:43:34.235255     1 net.cpp:219] drop7 does not need backward computation.\n",
      "I0425 12:43:34.235301     1 net.cpp:219] relu7 does not need backward computation.\n",
      "I0425 12:43:34.235345     1 net.cpp:219] fc7 does not need backward computation.\n",
      "I0425 12:43:34.235395     1 net.cpp:219] drop6 does not need backward computation.\n",
      "I0425 12:43:34.235412     1 net.cpp:219] relu6 does not need backward computation.\n",
      "I0425 12:43:34.235420     1 net.cpp:219] fc6 does not need backward computation.\n",
      "I0425 12:43:34.235427     1 net.cpp:219] pool5 does not need backward computation.\n",
      "I0425 12:43:34.235435     1 net.cpp:219] relu5 does not need backward computation.\n",
      "I0425 12:43:34.235445     1 net.cpp:219] conv5 does not need backward computation.\n",
      "I0425 12:43:34.235453     1 net.cpp:219] relu4 does not need backward computation.\n",
      "I0425 12:43:34.235461     1 net.cpp:219] conv4 does not need backward computation.\n",
      "I0425 12:43:34.235469     1 net.cpp:219] relu3 does not need backward computation.\n",
      "I0425 12:43:34.235505     1 net.cpp:219] conv3 does not need backward computation.\n",
      "I0425 12:43:34.235513     1 net.cpp:219] norm2 does not need backward computation.\n",
      "I0425 12:43:34.235522     1 net.cpp:219] pool2 does not need backward computation.\n",
      "I0425 12:43:34.235529     1 net.cpp:219] relu2 does not need backward computation.\n",
      "I0425 12:43:34.235540     1 net.cpp:219] conv2 does not need backward computation.\n",
      "I0425 12:43:34.235548     1 net.cpp:219] norm1 does not need backward computation.\n",
      "I0425 12:43:34.235560     1 net.cpp:219] pool1 does not need backward computation.\n",
      "I0425 12:43:34.235574     1 net.cpp:219] relu1 does not need backward computation.\n",
      "I0425 12:43:34.235586     1 net.cpp:219] conv1 does not need backward computation.\n",
      "I0425 12:43:34.235594     1 net.cpp:219] data does not need backward computation.\n",
      "I0425 12:43:34.235608     1 net.cpp:261] This network produces output prob\n",
      "I0425 12:43:34.235635     1 net.cpp:274] Network initialization done.\n",
      "I0425 12:43:37.218194     1 upgrade_proto.cpp:43] Attempting to upgrade input file specified using deprecated transformation parameters: models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel\n",
      "I0425 12:43:37.218287     1 upgrade_proto.cpp:46] Successfully upgraded file specified using deprecated data transformation parameters.\n",
      "W0425 12:43:37.218307     1 upgrade_proto.cpp:48] Note that future Caffe releases will only support transform_param messages for transformation fields.\n",
      "I0425 12:43:37.218361     1 upgrade_proto.cpp:52] Attempting to upgrade input file specified using deprecated V1LayerParameter: models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel\n",
      "I0425 12:43:37.451746     1 upgrade_proto.cpp:60] Successfully upgraded file specified using deprecated V1LayerParameter\n",
      "I0425 12:43:37.526979     1 net.cpp:752] Ignoring source layer loss\n",
      "mean-subtracted values: [('B', 104.0069879317889), ('G', 116.66876761696767), ('R', 122.6789143406786)]\n",
      "predicted class is: 211\n",
      "=== output label ===\n",
      "n02100583 vizsla, Hungarian pointer\n"
     ]
    }
   ],
   "source": [
    "!cd caffe;bash run_docker_python.sh classification.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
